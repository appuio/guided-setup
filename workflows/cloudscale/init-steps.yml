steps:
- match: Given I have all prerequisites installed
  description: |-
    This step checks if all necessary prerequisites are installed on your system,
    including 'yq' (version 4 or higher, by Mike Farah) and 'oc' (OpenShift CLI).
  run: |
    set -euo pipefail
    echo "Checking prerequisites..."

    which yq >/dev/null 2>&1 && { echo "✅ yq is installed."; } || { echo "❌ yq is not installed. Please install yq to proceed."; exit 1; }
    yq --version | grep -E 'version v[4-9]\.' | grep 'mikefarah' >/dev/null 2>&1 && { echo "✅ yq by mikefarah version 4 or higher is installed."; } || { echo "❌ yq version 4 or higher is required. Please upgrade yq to proceed."; exit 1; }

    which jq >/dev/null 2>&1 && { echo "✅ jq is installed."; } || { echo "❌ jq is not installed. Please install jq to proceed."; exit 1; }

    which oc >/dev/null 2>&1 && { echo "✅ oc (OpenShift CLI) is installed."; } || { echo "❌ oc (OpenShift CLI) is not installed. Please install oc to proceed."; exit 1; }

    which vault >/dev/null 2>&1 && { echo "✅ vault (HashiCorp Vault) is installed."; } || { echo "❌ vault (HashiCorp Vault) is not installed. Please install vault to proceed."; exit 1; }

    which curl >/dev/null 2>&1 && { echo "✅ curl is installed."; } || { echo "❌ curl is not installed. Please install curl to proceed."; exit 1; }

    which docker >/dev/null 2>&1 && { echo "✅ docker is installed."; } || { echo "❌ docker is not installed. Please install docker to proceed."; exit 1; }

    which glab >/dev/null 2>&1 && { echo "✅ glab (GitLab CLI) is installed."; } || { echo "❌ glab (GitLab CLI) is not installed. Please install glab to proceed."; exit 1; }

    which host >/dev/null 2>&1 && { echo "✅ host (DNS lookup utility) is installed."; } || { echo "❌ host (DNS lookup utility) is not installed. Please install host to proceed."; exit 1; }

    which mc >/dev/null 2>&1 && { echo "✅ mc (MinIO Client) is installed."; } || { echo "❌ mc (MinIO Client) is not installed. Please install mc >= RELEASE.2024-01-18T07-03-39Z to proceed."; exit 1; }
    mc_version=$(mc --version | grep -Eo 'RELEASE[^ ]+')
    echo "$mc_version" | grep -E 'RELEASE\.202[4-9]-' >/dev/null 2>&1 && { echo "✅ mc version ${mc_version} is sufficient."; } || { echo "❌ mc version ${mc_version} is insufficient. Please upgrade mc to >= RELEASE.2024-01-18T07-03-39Z to proceed."; exit 1; }

    which aws >/dev/null 2>&1 && { echo "✅ aws (AWS CLI) is installed."; } || { echo "❌ aws (AWS CLI) is not installed. Please install aws to proceed. Our recommended installer is uv: 'uv tool install awscli'"; exit 1; }

    which restic >/dev/null 2>&1 && { echo "✅ restic (Backup CLI) is installed."; } || { echo "❌ restic (Backup CLI) is not installed. Please install restic to proceed."; exit 1; }

    which emergency-credentials-receive >/dev/null 2>&1 && { echo "✅ emergency-credentials-receive (Cluster emergency access helper) is installed."; } || { echo "❌ emergency-credentials-receive is not installed. Please install it from https://github.com/vshn/emergency-credentials-receive ."; exit 1; }

    echo "✅ All prerequisites are met."
- match: And I have the `openshift-install` binary for version "(?P<ocp_version>[^"]+)"
  description: |-
    This step checks if the `openshift-install` binary for the specified OpenShift version is available in your PATH.

    If not found, it provides instructions on how to download it.
  run: |
    set -euo pipefail

    if command -v openshift-install >/dev/null 2>&1; then
      INSTALLED_VERSION=$(openshift-install version | grep 'openshift-install' | awk '{print $2}' | sed 's/^v//' | sed -E 's/\.[0-9]{1,2}$//')
      if [ "$INSTALLED_VERSION" = "$MATCH_ocp_version" ]; then
        echo "✅ openshift-install version ${MATCH_ocp_version}.XX is installed."
        exit 0
      else
        echo "❌ openshift-install version $INSTALLED_VERSION is installed, but version $MATCH_ocp_version is required. Please download the openshift-install binary for version $MATCH_ocp_version from https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-${MATCH_ocp_version}/ and add it to your PATH."
        exit 1
      fi
    else
      echo "❌ openshift-install binary not found in PATH. Please download the openshift-install binary for version $MATCH_ocp_version"
      echo "from https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-${MATCH_ocp_version}/ and add it to your PATH."
      exit 1
    fi
- match: And a lieutenant cluster
  description: |-
    This step retrieves the Commodore tenant ID associated with the given lieutenant cluster ID.

    Use https://api.syn.vshn.net as the Commodore API URL for production clusters.
    You might use the WebUI at https://control.vshn.net/syn/lieutenantapiendpoints to create and manage your clusters.

    For customer clusters ensure the following facts are set:
    * sales_order: Name of the sales order to which the cluster is billed, such as S10000
    * service_level: Name of the service level agreement for this cluster, such as guaranteed-availability
    * access_policy: Access-Policy of the cluster, such as regular or swissonly
    * release_channel: Name of the syn component release channel to use, such as stable
    * maintenance_window: Pick the appropriate upgrade schedule, such as monday-1400 for test clusters, tuesday-1000 for prod or custom to not (yet) enable maintenance
    * cilium_addons: Comma-separated list of cilium addons the customer gets billed for, such as advanced_networking or tetragon. Set to NONE if no addons should be billed.

    This step checks that you have access to the Commodore API and the cluster ID is valid.
  inputs:
  - name: commodore_api_url
    description: |-
      URL of the Commodore API to use for retrieving cluster information.

      Use https://api.syn.vshn.net as the Commodore API URL for production clusters.
      Use https://api-int.syn.vshn.net for test clusters.

      You might use the WebUI at https://control.vshn.net/syn/lieutenantapiendpoints to create and manage your clusters.

  - name: commodore_cluster_id
    description: |-
      Project Syn cluster ID for the cluster to be set up.

      In the form of c-example-infra-prod1.

      You might use the WebUI at https://control.vshn.net/syn/lieutenantapiendpoints to create and manage your clusters.
  outputs:
  - name: commodore_tenant_id
  - name: cloudscale_region
  run: |
    set -euo pipefail
    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    echo "Retrieving Commodore tenant ID for cluster ID '$INPUT_commodore_cluster_id' from API at '$INPUT_commodore_api_url'..."
    tenant_id=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${COMMODORE_API_URL}/clusters/${INPUT_commodore_cluster_id} | jq -r .tenant)
    echo "$tenant_id" | grep 't-' >/dev/null 2>&1 && { echo "✅ Retrieved tenant ID '$tenant_id' for cluster ID '$INPUT_commodore_cluster_id'."; } || { echo "❌ Failed to retrieve valid tenant ID for cluster ID '$INPUT_commodore_cluster_id'. Got '$tenant_id'. Please check your Commodore API access and cluster ID."; exit 1; }
    env -i "commodore_tenant_id=$tenant_id" >> $OUTPUT

    export region=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${COMMODORE_API_URL}/clusters/${INPUT_commodore_cluster_id} | jq -r .facts.region)
    test -z "$region" && test "$region" != "null" && { echo "❌ Failed to retrieve cloudscale region for cluster ID '$INPUT_commodore_cluster_id'."; exit 1; } || { echo "✅ Retrieved cloudscale region '$region' for cluster ID '$INPUT_commodore_cluster_id'."; }
    env -i "cloudscale_region=$region" >> $OUTPUT
- match: And a Keycloak service
  description: |-
    In this step, you have to create a Keycloak service for the new cluster
    via the VSHN Control Web UI at https://control.vshn.net/vshn/services/_create 
  inputs:
  - name: commodore_cluster_id
  run: |
    echo '#########################################################'
    echo '#                                                       #'
    echo '#  Please create a Keycloak service with the cluster's  #'
    echo '#  ID as Service Name via the VSHN Control Web UI.      #'
    echo '#                                                       #'
    echo '#########################################################'
    echo
    echo "The name and ID of the service should be ${INPUT_commodore_cluster_id}."
    echo "You can go to https://control.vshn.net/vshn/services/_create"
    sleep 2


- match: And Cloudscale API tokens
  inputs:
  - name: cloudscale_token
    description: |-
      Cloudscale API token with read+write permissions.

      Used for setting up the cluster and for the machine api provider.
  - name: cloudscale_token_floaty
    description: |-
      Cloudscale API token with read+write permissions.

      Used for managing the floating IPs.
  description: |-
    Create 2 new cloudscale API tokens with read+write permissions and name them
    <cluster_id> and <cluster_id>_floaty on control.cloudscale.ch/service/<your-project>/api-token.

    This step currently does not validate whether the tokens have read permission.
  run: |
    set -euo pipefail
    if [[ `curl -sH "Authorization: Bearer ${INPUT_cloudscale_token}" https://api.cloudscale.ch/v1/flavors -o /dev/null -w"%{http_code}"` != 200 ]]
    then
      echo "Cloudscale token not valid!"
    fi
    if [[ `curl -sH "Authorization: Bearer ${INPUT_cloudscale_token_floaty}" https://api.cloudscale.ch/v1/flavors -o /dev/null -w"%{http_code}"` != 200 ]]
    then
      echo "Cloudscale Floaty token not valid!"
    fi
- match: And a personal VSHN GitLab access token
  description: |-
    This step ensures that you have provided a personal access token for VSHN GitLab.

    Create the token at https://git.vshn.net/-/user_settings/personal_access_tokens with the "api" scope.

    This step currently does not validate the token's scope.
  inputs:
  - name: gitlab_api_token
    description: |-
      Personal access token for VSHN GitLab with the "api" scope.

      Create the token at https://git.vshn.net/-/user_settings/personal_access_tokens with the "api" scope.
  outputs:
  - name: gitlab_user_name
    description: |-
      Your GitLab user name.
  run: |
    set -euo pipefail
    user="$( curl -sH "Authorization: Bearer ${INPUT_gitlab_api_token}" "https://git.vshn.net/api/v4/user" | jq -r .username )"
    if [[ "$user" == "null" ]]
    then
      echo "Error validating GitLab token. Are you sure it is valid?"
      exit 1
    fi
    env -i "gitlab_user_name=$user" >> $OUTPUT
    echo "Token is valid."
- match: And a control.vshn.net Servers API token
  description: |-
    This step ensures that you have provided an API token for control.vshn.net Servers API.

    Create the token at https://control.vshn.net/tokens/_create/servers and ensure your IP is allowlisted.
  inputs:
  - name: control_vshn_api_token
    description: |-
      API token for control.vshn.net Servers API.

      Used to create the puppet based LBs.

      Be extra careful with the IP allowlist.
  run: |
    set -euo pipefail

    AUTH="X-AccessToken: ${INPUT_control_vshn_api_token}"

    code="$( curl -H"$AUTH" https://control.vshn.net/api/servers/1/appuio/ -o /dev/null -w"%{http_code}" )"

    if [[ "$code" != 200 ]]
    then
      echo "ERROR: could not access Server API (Status $code)"
      echo "Please ensure your token is valid and your IP is on the allowlist."
      exit 1
    fi
- match: And basic cluster information
  description: |-
    This step collects two essential pieces of information required for cluster setup: the base domain and the Red Hat pull secret.

    See https://kb.vshn.ch/oc4/explanations/dns_scheme.html for more information about the base domain.
    Get a pull secret from https://cloud.redhat.com/openshift/install/pull-secret.
  inputs:
  - name: base_domain
    description: |-
      The base domain for the cluster without the cluster ID prefix and the last dot.

      Example: `appuio-beta.ch`

      See https://kb.vshn.ch/oc4/explanations/dns_scheme.html for more information about the base domain.
  - name: redhat_pull_secret
    description: |-
      Red Hat pull secret for accessing Red Hat container images.

      Get a pull secret from https://cloud.redhat.com/openshift/install/pull-secret.
- match: Then I set secrets in Vault
  description: |-
    This step stores the collected secrets and tokens in the ProjectSyn Vault.
  inputs:
  - name: vault_address
    description: |-
      Address of the Vault server associated with the Lieutenant API to store cluster secrets.

      https://vault-prod.syn.vshn.net/ for production clusters.
  - name: commodore_cluster_id
  - name: commodore_tenant_id
  - name: bucket_user
  - name: cloudscale_token
  - name: cloudscale_token_floaty
  outputs:
  - name: hieradata_repo_user
  - name: hieradata_repo_token
  run: |
    set -euo pipefail

    export VAULT_ADDR=${INPUT_vault_address}
    vault login -method=oidc

    # Set the cloudscale.ch access secrets
    vault kv put clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/cloudscale \
      token=${INPUT_cloudscale_token} \
      s3_access_key=$(echo "${INPUT_bucket_user}" | jq -r '.keys[0].access_key') \
      s3_secret_key=$(echo "${INPUT_bucket_user}" | jq -r '.keys[0].secret_key')

    # Put LB API key in Vault
    vault kv put clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/floaty \
      iam_secret=${INPUT_cloudscale_token_floaty}

    # Generate an HTTP secret for the registry
    vault kv put clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/registry \
      httpSecret=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 128)

    # Generate a master password for K8up backups
    vault kv put clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/global-backup \
      password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

    # Generate a password for the cluster object backups
    vault kv put clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/cluster-backup \
      password=$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)

    hieradata_repo_secret=$(vault kv get \
      -format=json "clusters/kv/lbaas/hieradata_repo_token" | jq '.data.data')
    env -i "hieradata_repo_user=$(echo "${hieradata_repo_secret}" | jq -r '.user')" >> $OUTPUT
    env -i "hieradata_repo_token=$(echo "${hieradata_repo_secret}" | jq -r '.token')" >> $OUTPUT

- match: And I check the cluster domain
  description: |-
    Please verify that the base domain generated is correct for your setup.
  inputs:
  - name: commodore_cluster_id
  - name: base_domain
  outputs:
  - name: cluster_domain
  run: |
    set -euo pipefail

    cluster_domain="${INPUT_commodore_cluster_id}.${INPUT_base_domain}"
    echo "Cluster domain is set to '$cluster_domain'"
    echo "cluster_domain=$cluster_domain" >> $OUTPUT

- match: And I prepare the cluster repository
  description: |-
    This step prepares the local cluster repository by cloning the Commodore hieradata repository
    and setting up the necessary configuration for the specified cluster.
  inputs:
  - name: commodore_api_url
  - name: commodore_cluster_id
  - name: commodore_tenant_id
  - name: hieradata_repo_user
  - name: cluster_domain
  - name: hieradata_repo_token
  - name: image_major
  - name: image_minor
  run: |
    set -euo pipefail

    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    rm -rf inventory/classes/
    mkdir -p inventory/classes/
    git clone $(curl -sH"Authorization: Bearer $(commodore fetch-token)" "${INPUT_commodore_api_url}/tenants/${INPUT_commodore_tenant_id}" | jq -r '.gitRepo.url') inventory/classes/${INPUT_commodore_tenant_id}

    pushd "inventory/classes/${INPUT_commodore_tenant_id}/"

    yq eval -i ".parameters.openshift.baseDomain = \"${INPUT_cluster_domain}\"" \
      ${INPUT_commodore_cluster_id}.yml

    git diff --exit-code --quiet || git commit -a -m "Configure cluster domain for ${INPUT_commodore_cluster_id}"

    if ls openshift4.y*ml 1>/dev/null 2>&1; then
      yq eval -i '.classes += ".openshift4"' ${INPUT_commodore_cluster_id}.yml;
      git diff --exit-code --quiet || git commit -a -m "Include openshift4 class for ${INPUT_commodore_cluster_id}"
    fi

    yq eval -i '.parameters.openshift.cloudscale.subnet_uuid = "TO_BE_DEFINED"' ${INPUT_commodore_cluster_id}.yml

    yq eval -i '.parameters.openshift.cloudscale.rhcos_image_slug = "rhcos-4.19"' \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift4_terraform.terraform_variables.ignition_ca = \"TO_BE_DEFINED\"" \
      ${INPUT_commodore_cluster_id}.yml

    git diff --exit-code --quiet || git commit -a -m "Configure Cloudscale metaparameters on ${INPUT_commodore_cluster_id}"

    yq eval -i '.applications += ["cloudscale-loadbalancer-controller"]' ${INPUT_commodore_cluster_id}.yml
    yq eval -i '.applications = (.applications | unique)' ${INPUT_commodore_cluster_id}.yml
    cat ${INPUT_commodore_cluster_id}.yml

    git diff --exit-code --quiet || git commit -a -m "Enable cloudscale loadbalancer controller for ${INPUT_commodore_cluster_id}"

    yq eval -i '.applications += ["cilium"]' ${INPUT_commodore_cluster_id}.yml
    yq eval -i '.applications = (.applications | unique)' ${INPUT_commodore_cluster_id}.yml

    yq eval -i '.parameters.networkpolicy.networkPlugin = "cilium"' ${INPUT_commodore_cluster_id}.yml

    yq eval -i '.parameters.openshift4_monitoring.upstreamRules.networkPlugin = "cilium"' ${INPUT_commodore_cluster_id}.yml
    yq eval -i '.parameters.openshift.infraID = "TO_BE_DEFINED"' ${INPUT_commodore_cluster_id}.yml
    yq eval -i '.parameters.openshift.clusterID = "TO_BE_DEFINED"' ${INPUT_commodore_cluster_id}.yml

    git diff --exit-code --quiet || git commit -a -m "Add Cilium addon to ${INPUT_commodore_cluster_id}"

    git push

    popd

    commodore catalog compile ${INPUT_commodore_cluster_id} --push \
      --dynamic-fact kubernetesVersion.major=1 \
      --dynamic-fact kubernetesVersion.minor="$((${INPUT_image_minor}+13))" \
      --dynamic-fact openshiftVersion.Major=${INPUT_image_major} \
      --dynamic-fact openshiftVersion.Minor=${INPUT_image_minor}

- match: Then I configure the OpenShift installer
  description: |-
    This step configures the OpenShift installer for the Cloudscale cluster by generating
    the necessary installation files using Commodore.
  inputs:
  - name: commodore_cluster_id
  - name: commodore_tenant_id
  - name: base_domain
  - name: cluster_domain
  - name: vault_address
  - name: redhat_pull_secret
  - name: cloudscale_region
  - name: bucket_user
  - name: cloudscale_token
  outputs:
  - name: ignition_bootstrap
  - name: ssh_public_key_path
  run: |
    set -euo pipefail

    export VAULT_ADDR=${INPUT_vault_address}
    vault login -method=oidc

    ssh_private_key="$(pwd)/ssh_${INPUT_commodore_cluster_id}"
    ssh_public_key="${ssh_private_key}.pub"

    env -i "ssh_public_key_path=$ssh_public_key" >> $OUTPUT

    if vault kv get -format=json clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/cloudscale/ssh >/dev/null 2>&1; then
      echo "SSH keypair for cluster ${INPUT_commodore_cluster_id} already exists in Vault, skipping generation."

      vault kv get -format=json clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/cloudscale/ssh | \
        jq -r '.data.data.private_key|@base64d' > ${ssh_private_key}

      chmod 600 ${ssh_private_key}
      ssh-keygen -f ${ssh_private_key} -y > ${ssh_public_key}

    else
      echo "Generating new SSH keypair for cluster ${INPUT_commodore_cluster_id}."

      ssh-keygen -C "vault@${INPUT_commodore_cluster_id}" -t ed25519 -f $ssh_private_key -N ''

      base64_no_wrap='base64'
      if [[ "$OSTYPE" == "linux"* ]]; then
        base64_no_wrap='base64 --wrap 0'
      fi

      vault kv put clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/cloudscale/ssh \
        private_key=$(cat $ssh_private_key | eval "$base64_no_wrap")
    fi

    echo Adding SSH private key to ssh-agent...
    echo You might need to start the ssh-agent first using: eval "\$(ssh-agent)"
    echo +ssh-add $ssh_private_key
    ssh-add $ssh_private_key

    installer_dir="$(pwd)/target"
    rm -rf "${installer_dir}"
    mkdir -p "${installer_dir}"

    cat > "${installer_dir}/install-config.yaml" <<EOF
    apiVersion: v1
    metadata:
      name: ${INPUT_commodore_cluster_id}
    baseDomain: ${INPUT_base_domain}
    platform:
      external:
        platformName: cloudscale
        cloudControllerManager: External
    networking:
      networkType: Cilium
    pullSecret: |
      ${INPUT_redhat_pull_secret}
    sshKey: "$(cat $ssh_public_key)"
    EOF

    echo Running OpenShift installer to create manifests...
    openshift-install --dir "${installer_dir}" create manifests

    echo Copying machineconfigs...
    machineconfigs=catalog/manifests/openshift4-nodes/10_machineconfigs.yaml
    if [ -f $machineconfigs ];  then
      yq --no-doc -s \
        "\"${installer_dir}/openshift/99x_openshift-machineconfig_\" + .metadata.name" \
        $machineconfigs
    fi

    echo Copying Cloudscale CCM manifests...
    for f in catalog/manifests/cloudscale-cloud-controller-manager/*; do
      cp $f ${installer_dir}/manifests/cloudscale_ccm_$(basename $f)
    done
    yq -i e ".stringData.access-token=\"${INPUT_cloudscale_token}\"" \
      ${installer_dir}/manifests/cloudscale_ccm_01_secret.yaml

    echo Copying Cilium OLM manifests...
    cp catalog/manifests/cilium/olm/* ${installer_dir}/manifests/

    gen_cluster_domain=$(yq e '.spec.baseDomain' \
      "${installer_dir}/manifests/cluster-dns-02-config.yml")
    if [ "$gen_cluster_domain" != "$INPUT_cluster_domain" ]; then
      echo -e "\033[0;31mGenerated cluster domain doesn't match expected cluster domain: Got '$gen_cluster_domain', want '$INPUT_cluster_domain'\033[0;0m"
      exit 1
    else
      echo -e "\033[0;32mGenerated cluster domain matches expected cluster domain.\033[0;0m"
    fi

    echo Running OpenShift installer to create ignition configs...
    openshift-install --dir "${installer_dir}" \
      create ignition-configs

    mc alias set \
      "${INPUT_commodore_cluster_id}" "https://objects.${INPUT_cloudscale_region}.cloudscale.ch" \
      $(echo "$INPUT_bucket_user" | jq -r '.keys[0].access_key') \
      $(echo "$INPUT_bucket_user" | jq -r '.keys[0].secret_key')

    mc cp "${installer_dir}/bootstrap.ign" "${INPUT_commodore_cluster_id}/${INPUT_commodore_cluster_id}-bootstrap-ignition/"

    ignition_bootstrap=$(mc share download \
      --json --expire=4h \
      "${INPUT_commodore_cluster_id}/${INPUT_commodore_cluster_id}-bootstrap-ignition/bootstrap.ign" | jq -r '.share')

    env -i "ignition_bootstrap=$ignition_bootstrap" >> $OUTPUT

    echo "✅ OpenShift installer configured successfully."

- match: And I configure Terraform for team "(?P<team_name>[^"]+)"
  description: |-
    This step configures Terraform the Commodore rendered terraform configuration.
  inputs:
  - name: commodore_api_url
  - name: commodore_cluster_id
  - name: commodore_tenant_id
  - name: cloudscale_region
  - name: ssh_public_key_path
  - name: hieradata_repo_user
  - name: base_domain
  - name: image_major
  - name: image_minor
  run: |
    set -euo pipefail

    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    installer_dir="$(pwd)/target"

    pushd "inventory/classes/${INPUT_commodore_tenant_id}/"

    yq eval -i '.classes += ["global.distribution.openshift4.no-opsgenie"]' ${INPUT_commodore_cluster_id}.yml;
    yq eval -i '.classes = (.classes | unique)' ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift.infraID = \"$(jq -r .infraID "${installer_dir}/metadata.json")\"" \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift.clusterID = \"$(jq -r .clusterID "${installer_dir}/metadata.json")\"" \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift.ssh_key = \"$(cat ${INPUT_ssh_public_key_path})\"" \
      ${INPUT_commodore_cluster_id}.yml

    ca_cert=$(jq -r '.ignition.security.tls.certificateAuthorities[0].source' \
      "${installer_dir}/master.ign" | \
      awk -F ',' '{ print $2 }' | \
      base64 --decode)

    yq eval -i ".parameters.openshift4_terraform.terraform_variables.base_domain = \"${INPUT_base_domain}\"" \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift4_terraform.terraform_variables.ignition_ca = \"${ca_cert}\"" \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift4_terraform.terraform_variables.ssh_keys = [\"$(cat ${INPUT_ssh_public_key_path})\"]" \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift4_terraform.terraform_variables.allocate_router_vip_for_lb_controller = true" \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift4_terraform.terraform_variables.team = \"${MATCH_team_name}\"" \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift4_terraform.terraform_variables.hieradata_repo_user = \"${INPUT_hieradata_repo_user}\"" \
      ${INPUT_commodore_cluster_id}.yml

    git commit -a -m "Setup cluster ${INPUT_commodore_cluster_id}"
    git push

    popd

    commodore catalog compile ${INPUT_commodore_cluster_id} --push \
      --dynamic-fact kubernetesVersion.major=1 \
      --dynamic-fact kubernetesVersion.minor="$((${INPUT_image_minor}+13))" \
      --dynamic-fact openshiftVersion.Major=${INPUT_image_major} \
      --dynamic-fact openshiftVersion.Minor=${INPUT_image_minor}

- match: Then I provision the loadbalancers
  description: |-
    This step provisions the load balancers for the Cloudscale OpenShift cluster using Terraform.
  inputs:
  - name: cloudscale_token
  - name: cloudscale_token_floaty
  - name: control_vshn_api_token
  - name: ignition_bootstrap
  - name: hieradata_repo_token
  - name: gitlab_user_name
  - name: gitlab_api_token
  - name: commodore_cluster_id
  - name: commodore_tenant_id
  - name: commodore_api_url
  - name: cluster_domain
  outputs:
  - name: lb_fqdn_1
  - name: lb_fqdn_2
  run: |
    set -euo pipefail

    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    cat <<EOF > ./terraform.env
    CLOUDSCALE_API_TOKEN=${INPUT_cloudscale_token}
    TF_VAR_ignition_bootstrap=${INPUT_ignition_bootstrap}
    TF_VAR_lb_cloudscale_api_secret=${INPUT_cloudscale_token_floaty}
    TF_VAR_control_vshn_net_token=${INPUT_control_vshn_api_token}
    GIT_AUTHOR_NAME=$(git config --global user.name)
    GIT_AUTHOR_EMAIL=$(git config --global user.email)
    HIERADATA_REPO_TOKEN=${INPUT_hieradata_repo_token}
    EOF

    tf_image=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.image" \
      dependencies/openshift4-terraform/class/defaults.yml)
    tf_tag=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.tag" \
      dependencies/openshift4-terraform/class/defaults.yml)

    echo "Using Terraform image: ${tf_image}:${tf_tag}"

    base_dir=$(pwd)
    alias terraform='touch .terraformrc; docker run --rm -e REAL_UID=$(id -u) -e TF_CLI_CONFIG_FILE=/tf/.terraformrc --env-file ${base_dir}/terraform.env -w /tf -v $(pwd):/tf --ulimit memlock=-1 "${tf_image}:${tf_tag}" /tf/terraform.sh'

    gitlab_repository_url=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${INPUT_commodore_api_url}/clusters/${INPUT_commodore_cluster_id} | jq -r '.gitRepo.url' | sed 's|ssh://||; s|/|:|')
    gitlab_repository_name=${gitlab_repository_url##*/}
    gitlab_catalog_project_id=$(curl -sH "Authorization: Bearer ${INPUT_gitlab_api_token}" "https://git.vshn.net/api/v4/projects?simple=true&search=${gitlab_repository_name/.git}" | jq -r ".[] | select(.ssh_url_to_repo == \"${gitlab_repository_url}\") | .id")
    gitlab_state_url="https://git.vshn.net/api/v4/projects/${gitlab_catalog_project_id}/terraform/state/cluster"

    pushd catalog/manifests/openshift4-terraform/

    terraform init \
      "-backend-config=address=${gitlab_state_url}" \
      "-backend-config=lock_address=${gitlab_state_url}/lock" \
      "-backend-config=unlock_address=${gitlab_state_url}/lock" \
      "-backend-config=username=${INPUT_gitlab_user_name}" \
      "-backend-config=password=${INPUT_gitlab_api_token}" \
      "-backend-config=lock_method=POST" \
      "-backend-config=unlock_method=DELETE" \
      "-backend-config=retry_wait_min=5"

    cat > override.tf <<EOF
    module "cluster" {
      bootstrap_count          = 0
      master_count             = 0
      infra_count              = 0
      worker_count             = 0
      additional_worker_groups = {}
    }
    EOF
    terraform apply -auto-approve -target "module.cluster.module.lb.module.hiera"

    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    echo "@                                                                                       @"
    echo "@  Please review and merge the LB hieradata MR listed in Terraform output hieradata_mr. @"
    echo "@                                                                                       @"
    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    while (GITLAB_HOST=git.vshn.net GITLAB_TOKEN="${INPUT_gitlab_api_token}" glab mr list -R=appuio/appuio_hieradata | grep "${INPUT_commodore_cluster_id}")
    do
      sleep 10
    done
    echo PR merged, waiting for CI to finish...
    sleep 10
    while (GITLAB_HOST=git.vshn.net GITLAB_TOKEN="${INPUT_gitlab_api_token}" glab mr list -R=appuio/appuio_hieradata | grep "running")
    do
      sleep 10
    done

    terraform apply -auto-approve
    dnstmp=$(mktemp)
    terraform output -raw cluster_dns > "$dnstmp"
    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    echo "@                                                                                       @"
    echo "@  Please add the DNS records shown in the Terraform output to your DNS provider.       @"
    echo "@  Most probably in https://git.vshn.net/vshn/vshn_zonefiles                            @"
    echo "@                                                                                       @"
    echo "@  If terminal selection does not work the entries can also be copied from              @"
    echo "@    $dnstmp                                                                            @"
    echo "@                                                                                       @"
    echo "@  Waiting for record to propagate...                                                   @"
    echo "@                                                                                       @"
    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    while ! (host "api.${INPUT_cluster_domain}")
    do
      sleep 15
    done
    rm -f "$dnstmp"

    lb1=$(terraform state show "module.cluster.module.lb.cloudscale_server.lb[0]" | grep fqdn | awk '{print $2}' | tr -d ' "\r\n')
    lb2=$(terraform state show "module.cluster.module.lb.cloudscale_server.lb[1]" | grep fqdn | awk '{print $2}' | tr -d ' "\r\n')

    echo "Loadbalancer FQDNs: $lb1 , $lb2"

    echo "Waiting for HAproxy ..."
    while true; do
      curl --connect-timeout 1 "http://api.${INPUT_cluster_domain}:6443" &>/dev/null || exit_code=$?
      if [ $exit_code -eq 52 ]; then
        echo "  HAproxy up!"
        break
      else
        echo -n "."
        sleep 5
      fi
    done

    echo "updating ssh config..."
    ssh management2.corp.vshn.net "sshop --output-archive /dev/stdout" | tar -C ~ -xzf -
    echo done

    echo "waiting for ssh access ..."
    ssh "${lb1}" hostname -f
    ssh "${lb2}" hostname -f

    env -i "lb_fqdn_1=$lb1" >> $OUTPUT
    env -i "lb_fqdn_2=$lb2" >> $OUTPUT

- match: And I provision the bootstrap node
  description: |-
    This step provisions the bootstrap node for the Cloudscale OpenShift cluster using Terraform.
  inputs:
  - name: cloudscale_token
  - name: cloudscale_token_floaty
  - name: control_vshn_api_token
  - name: ignition_bootstrap
  - name: hieradata_repo_token
  - name: gitlab_user_name
  - name: gitlab_api_token
  - name: commodore_cluster_id
  - name: commodore_tenant_id
  - name: commodore_api_url
  - name: lb_fqdn_1
  - name: lb_fqdn_2
  outputs:
  - name: kubeconfig_path
  run: |
    set -euo pipefail

    export COMMODORE_API_URL="${INPUT_commodore_api_url}"
    installer_dir="$(pwd)/target"

    cat <<EOF > ./terraform.env
    CLOUDSCALE_API_TOKEN=${INPUT_cloudscale_token}
    TF_VAR_ignition_bootstrap=${INPUT_ignition_bootstrap}
    TF_VAR_lb_cloudscale_api_secret=${INPUT_cloudscale_token_floaty}
    TF_VAR_control_vshn_net_token=${INPUT_control_vshn_api_token}
    GIT_AUTHOR_NAME=$(git config --global user.name)
    GIT_AUTHOR_EMAIL=$(git config --global user.email)
    HIERADATA_REPO_TOKEN=${INPUT_hieradata_repo_token}
    EOF

    tf_image=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.image" \
      dependencies/openshift4-terraform/class/defaults.yml)
    tf_tag=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.tag" \
      dependencies/openshift4-terraform/class/defaults.yml)

    echo "Using Terraform image: ${tf_image}:${tf_tag}"

    base_dir=$(pwd)
    alias terraform='touch .terraformrc; docker run --rm -e REAL_UID=$(id -u) -e TF_CLI_CONFIG_FILE=/tf/.terraformrc --env-file ${base_dir}/terraform.env -w /tf -v $(pwd):/tf --ulimit memlock=-1 "${tf_image}:${tf_tag}" /tf/terraform.sh'

    gitlab_repository_url=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${INPUT_commodore_api_url}/clusters/${INPUT_commodore_cluster_id} | jq -r '.gitRepo.url' | sed 's|ssh://||; s|/|:|')
    gitlab_repository_name=${gitlab_repository_url##*/}
    gitlab_catalog_project_id=$(curl -sH "Authorization: Bearer ${INPUT_gitlab_api_token}" "https://git.vshn.net/api/v4/projects?simple=true&search=${gitlab_repository_name/.git}" | jq -r ".[] | select(.ssh_url_to_repo == \"${gitlab_repository_url}\") | .id")
    gitlab_state_url="https://git.vshn.net/api/v4/projects/${gitlab_catalog_project_id}/terraform/state/cluster"

    pushd catalog/manifests/openshift4-terraform/

    terraform init \
      "-backend-config=address=${gitlab_state_url}" \
      "-backend-config=lock_address=${gitlab_state_url}/lock" \
      "-backend-config=unlock_address=${gitlab_state_url}/lock" \
      "-backend-config=username=${INPUT_gitlab_user_name}" \
      "-backend-config=password=${INPUT_gitlab_api_token}" \
      "-backend-config=lock_method=POST" \
      "-backend-config=unlock_method=DELETE" \
      "-backend-config=retry_wait_min=5"

    cat > override.tf <<EOF
    module "cluster" {
      bootstrap_count          = 1
      master_count             = 0
      infra_count              = 0
      worker_count             = 0
      additional_worker_groups = {}
    }
    EOF
    terraform apply -auto-approve

    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    echo "@                                                                                       @"
    echo "@  Please review and merge the LB hieradata MR listed in Terraform output hieradata_mr. @"
    echo "@                                                                                       @"
    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    while (GITLAB_HOST=git.vshn.net GITLAB_TOKEN="${INPUT_gitlab_api_token}" glab mr list -R=appuio/appuio_hieradata | grep "${INPUT_commodore_cluster_id}")
    do
      sleep 10
    done
    echo PR merged, waiting for CI to finish...
    sleep 10
    while (GITLAB_HOST=git.vshn.net GITLAB_TOKEN="${INPUT_gitlab_api_token}" glab mr list -R=appuio/appuio_hieradata | grep "running")
    do
      sleep 10
    done

    ssh "${INPUT_lb_fqdn_1}" sudo puppetctl run
    ssh "${INPUT_lb_fqdn_2}" sudo puppetctl run

    echo -n "Waiting for Bootstrap API to become available .."
    API_URL=$(yq e '.clusters[0].cluster.server' "${installer_dir}/auth/kubeconfig")
    while ! curl --connect-timeout 1 "${API_URL}/healthz" -k &>/dev/null; do
      echo -n "."
      sleep 5
    done && echo "✅ API is up"

    env -i "kubeconfig_path=${installer_dir}/auth/kubeconfig" >> $OUTPUT

- match: And I store the subnet ID and floating IP in the Syn hierarchy
  description: |-
    This step retrieves the subnet ID and ingress floating IP from Terraform and
    stores them in the Syn hierarchy.
  inputs:
  - name: cloudscale_token
  - name: cloudscale_token_floaty
  - name: control_vshn_api_token
  - name: ignition_bootstrap
  - name: hieradata_repo_token
  - name: gitlab_user_name
  - name: gitlab_api_token
  - name: commodore_cluster_id
  - name: commodore_tenant_id
  - name: commodore_api_url
  - name: image_major
  - name: image_minor
  run: |
    set -euo pipefail
    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    cat <<EOF > ./terraform.env
    CLOUDSCALE_API_TOKEN=${INPUT_cloudscale_token}
    TF_VAR_ignition_bootstrap=${INPUT_ignition_bootstrap}
    TF_VAR_lb_cloudscale_api_secret=${INPUT_cloudscale_token_floaty}
    TF_VAR_control_vshn_net_token=${INPUT_control_vshn_api_token}
    GIT_AUTHOR_NAME=$(git config --global user.name)
    GIT_AUTHOR_EMAIL=$(git config --global user.email)
    HIERADATA_REPO_TOKEN=${INPUT_hieradata_repo_token}
    EOF

    tf_image=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.image" \
      dependencies/openshift4-terraform/class/defaults.yml)
    tf_tag=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.tag" \
      dependencies/openshift4-terraform/class/defaults.yml)

    echo "Using Terraform image: ${tf_image}:${tf_tag}"

    base_dir=$(pwd)
    alias terraform='touch .terraformrc; docker run --rm -e REAL_UID=$(id -u) -e TF_CLI_CONFIG_FILE=/tf/.terraformrc --env-file ${base_dir}/terraform.env -w /tf -v $(pwd):/tf --ulimit memlock=-1 "${tf_image}:${tf_tag}" /tf/terraform.sh'

    gitlab_repository_url=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${INPUT_commodore_api_url}/clusters/${INPUT_commodore_cluster_id} | jq -r '.gitRepo.url' | sed 's|ssh://||; s|/|:|')
    gitlab_repository_name=${gitlab_repository_url##*/}
    gitlab_catalog_project_id=$(curl -sH "Authorization: Bearer ${INPUT_gitlab_api_token}" "https://git.vshn.net/api/v4/projects?simple=true&search=${gitlab_repository_name/.git}" | jq -r ".[] | select(.ssh_url_to_repo == \"${gitlab_repository_url}\") | .id")
    gitlab_state_url="https://git.vshn.net/api/v4/projects/${gitlab_catalog_project_id}/terraform/state/cluster"

    pushd catalog/manifests/openshift4-terraform/

    terraform init \
      "-backend-config=address=${gitlab_state_url}" \
      "-backend-config=lock_address=${gitlab_state_url}/lock" \
      "-backend-config=unlock_address=${gitlab_state_url}/lock" \
      "-backend-config=username=${INPUT_gitlab_user_name}" \
      "-backend-config=password=${INPUT_gitlab_api_token}" \
      "-backend-config=lock_method=POST" \
      "-backend-config=unlock_method=DELETE" \
      "-backend-config=retry_wait_min=5"

    SUBNET_UUID="$(terraform output -raw subnet_uuid)"
    INGRESS_FLOATING_IP="$(terraform output -raw router_vip)"
    pushd ../../../inventory/classes/${INPUT_commodore_tenant_id}

    yq eval -i '.parameters.openshift.cloudscale.subnet_uuid = "'$SUBNET_UUID'"' \
      ${INPUT_commodore_cluster_id}.yml
    yq eval -i '.parameters.openshift.cloudscale.ingress_floating_ip_v4 = "'$INGRESS_FLOATING_IP'"' \
      ${INPUT_commodore_cluster_id}.yml

    if not git diff-index --quiet HEAD
    then
      git commit -am "Configure cloudscale subnet UUID and ingress floating IP for ${INPUT_commodore_cluster_id}"
      git push origin master
    fi || true

    popd
    popd # yes, twice.

    # Recompile the catalog
    commodore catalog compile ${INPUT_commodore_cluster_id} --push \
      --dynamic-fact kubernetesVersion.major=1 \
      --dynamic-fact kubernetesVersion.minor="$((${INPUT_image_minor}+13))" \
      --dynamic-fact openshiftVersion.Major=${INPUT_image_major} \
      --dynamic-fact openshiftVersion.Minor=${INPUT_image_minor}

- match: And I provision the control plane
  description: |-
    This step provisions the control plane nodes with Terraform.
  inputs:
  - name: cloudscale_token
  - name: cloudscale_token_floaty
  - name: control_vshn_api_token
  - name: ignition_bootstrap
  - name: hieradata_repo_token
  - name: gitlab_user_name
  - name: gitlab_api_token
  - name: commodore_cluster_id
  - name: commodore_tenant_id
  - name: commodore_api_url
  - name: kubeconfig_path
  - name: cluster_domain
  run: |
    set -euo pipefail

    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    cat <<EOF > ./terraform.env
    CLOUDSCALE_API_TOKEN=${INPUT_cloudscale_token}
    TF_VAR_ignition_bootstrap=${INPUT_ignition_bootstrap}
    TF_VAR_lb_cloudscale_api_secret=${INPUT_cloudscale_token_floaty}
    TF_VAR_control_vshn_net_token=${INPUT_control_vshn_api_token}
    GIT_AUTHOR_NAME=$(git config --global user.name)
    GIT_AUTHOR_EMAIL=$(git config --global user.email)
    HIERADATA_REPO_TOKEN=${INPUT_hieradata_repo_token}
    EOF

    tf_image=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.image" \
      dependencies/openshift4-terraform/class/defaults.yml)
    tf_tag=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.tag" \
      dependencies/openshift4-terraform/class/defaults.yml)

    echo "Using Terraform image: ${tf_image}:${tf_tag}"

    base_dir=$(pwd)
    alias terraform='touch .terraformrc; docker run --rm -e REAL_UID=$(id -u) -e TF_CLI_CONFIG_FILE=/tf/.terraformrc --env-file ${base_dir}/terraform.env -w /tf -v $(pwd):/tf --ulimit memlock=-1 "${tf_image}:${tf_tag}" /tf/terraform.sh'

    gitlab_repository_url=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${INPUT_commodore_api_url}/clusters/${INPUT_commodore_cluster_id} | jq -r '.gitRepo.url' | sed 's|ssh://||; s|/|:|')
    gitlab_repository_name=${gitlab_repository_url##*/}
    gitlab_catalog_project_id=$(curl -sH "Authorization: Bearer ${INPUT_gitlab_api_token}" "https://git.vshn.net/api/v4/projects?simple=true&search=${gitlab_repository_name/.git}" | jq -r ".[] | select(.ssh_url_to_repo == \"${gitlab_repository_url}\") | .id")
    gitlab_state_url="https://git.vshn.net/api/v4/projects/${gitlab_catalog_project_id}/terraform/state/cluster"

    pushd catalog/manifests/openshift4-terraform/

    terraform init \
      "-backend-config=address=${gitlab_state_url}" \
      "-backend-config=lock_address=${gitlab_state_url}/lock" \
      "-backend-config=unlock_address=${gitlab_state_url}/lock" \
      "-backend-config=username=${INPUT_gitlab_user_name}" \
      "-backend-config=password=${INPUT_gitlab_api_token}" \
      "-backend-config=lock_method=POST" \
      "-backend-config=unlock_method=DELETE" \
      "-backend-config=retry_wait_min=5"

    cat > override.tf <<EOF
    module "cluster" {
      bootstrap_count          = 1
      infra_count              = 0
      worker_count             = 0
      additional_worker_groups = {}
    }
    EOF

    echo "Patching Cilium manifests so control plane bootstrap can proceed ..."

    export KUBECONFIG="${INPUT_kubeconfig_path}"

    while ! kubectl get ciliumconfig -A &>/dev/null; do
      echo -n "."
      sleep 2
    done && echo -e "\nCiliumConfig CR is present"

    kubectl patch -n cilium ciliumconfig cilium-enterprise --type=merge \
    -p '{
      "spec": {
        "cilium": {
          "kubeProxyReplacement": "false",
          "nodePort": {
            "enabled": true
          },
          "socketLB": {
            "enabled": true
          },
          "sessionAffinity": true,
          "externalIPs": {
            "enabled": true
          },
          "hostPort": {
            "enabled": true
          }
        }
      }
    }'

    echo "Running Terraform ..."

    terraform apply -auto-approve
    dnstmp=$(mktemp)
    terraform output -raw cluster_dns > "$dnstmp"
    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    echo "@                                                                                       @"
    echo "@  Please add the etcd DNS records shown in the Terraform output to your DNS provider.  @"
    echo "@  Most probably in https://git.vshn.net/vshn/vshn_zonefiles                            @"
    echo "@                                                                                       @"
    echo "@  If terminal selection does not work the entries can also be copied from              @"
    echo "@    $dnstmp                                                                            @"
    echo "@                                                                                       @"
    echo "@  Waiting for record to propagate...                                                   @"
    echo "@                                                                                       @"
    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    while ! (host "etcd-0.${INPUT_cluster_domain}")
    do
      sleep 15
    done
    rm -f "$dnstmp"

    echo "Waiting for masters to become ready ..."
    kubectl wait --for condition=ready node -l node-role.kubernetes.io/master
    popd

- match: Then I deploy initial manifests
  description: |-
    This step deploys some manifests required during bootstrap, including
    cert-manager,  machine-api-provider, machinesets, loadbalancer controller,
    and ingress loadbalancer.
  inputs:
  - name: commodore_cluster_id
  - name: commodore_tenant_id
  - name: commodore_api_url
  - name: vault_address
  - name: kubeconfig_path
  run: |
    set -euo pipefail
    export COMMODORE_API_URL="${INPUT_commodore_api_url}"
    installer_dir="$(pwd)/target"
    export KUBECONFIG="${INPUT_kubeconfig_path}"

    export VAULT_ADDR=${INPUT_vault_address}
    vault login -method=oidc

    echo '# Applying cert-manager ... #'
    kubectl apply -f catalog/manifests/cert-manager/00_namespace.yaml
    kubectl apply -Rf catalog/manifests/cert-manager/10_cert_manager
    kubectl -n syn-cert-manager patch --type=merge \
      $(kubectl -n syn-cert-manager get deploy -oname) \
      -p '{"spec":{"template":{"spec":{"tolerations":[{"operator":"Exists"}]}}}}'
    echo '# Applied cert-manager. #'
    echo
    echo '# Applying machine-api-provider ... #'
    export VAULT_TOKEN=$(vault token lookup -format=json | jq -r .data.id)
    kapitan refs --reveal --refs-path catalog/refs -f catalog/manifests/machine-api-provider-cloudscale/00_secrets.yaml | kubectl apply -f -
    kubectl apply  -f catalog/manifests/machine-api-provider-cloudscale/10_clusterRoleBinding.yaml
    kubectl apply -f catalog/manifests/machine-api-provider-cloudscale/10_serviceAccount.yaml
    kubectl apply -f catalog/manifests/machine-api-provider-cloudscale/11_deployment.yaml
    echo '# Applied machine-api-provider. #'
    echo
    echo '# Applying machinesets ... #'
    kubectl apply `for f in catalog/manifests/openshift4-nodes/machineset-*.yaml ; do echo -n "-f $f " ; done`
    echo '# Applied machinesets. #'
    echo
    echo '# Applying loadbalancer controller ... #'
    kubectl apply -f catalog/manifests/cloudscale-loadbalancer-controller/00_namespace.yaml
    kapitan refs --reveal --refs-path catalog/refs -f catalog/manifests/cloudscale-loadbalancer-controller/10_secrets.yaml | kubectl apply -f -

    # TODO(aa): This fails on the first attempt because likely some of the previous resources need time to come online; figure out what to wait for
    until kubectl apply -Rf catalog/manifests/cloudscale-loadbalancer-controller/10_kustomize
    do
      echo "Manifests didn't apply, waiting a moment to try again ..."
      sleep 20
    done
    echo "Waiting for load balancer to become available ..."
    kubectl -n appuio-cloudscale-loadbalancer-controller \
      wait --for condition=available \
      deploy cloudscale-loadbalancer-controller-controller-manager
    echo '# Applied loadbalancer controller. #'
    echo
    echo '# Applying ingress loadbalancer ... #'
    kubectl apply -f catalog/manifests/cloudscale-loadbalancer-controller/20_loadbalancers.yaml
    echo '# Applied ingress loadbalancer. #'
    echo

- match: And I wait for bootstrap to complete
  description: |-
    This step waits for OpenShift bootstrap to complete successfully.
  run: |
    set -euo pipefail
    installer_dir="$(pwd)/target"
    openshift-install --dir "${installer_dir}" \
      wait-for bootstrap-complete --log-level debug

- match: Then I remove the bootstrap node
  description: |-
    After successful bootstrapping, this step removes the bootstrap node again.
  inputs:
  - name: cloudscale_token
  - name: cloudscale_token_floaty
  - name: control_vshn_api_token
  - name: ignition_bootstrap
  - name: hieradata_repo_token
  - name: gitlab_user_name
  - name: gitlab_api_token
  - name: commodore_cluster_id
  - name: commodore_api_url
  - name: lb_fqdn_1
  - name: lb_fqdn_2
  run: |
    set -euo pipefail

    export COMMODORE_API_URL="${INPUT_commodore_api_url}"
    installer_dir="$(pwd)/target"

    cat <<EOF > ./terraform.env
    CLOUDSCALE_API_TOKEN=${INPUT_cloudscale_token}
    TF_VAR_ignition_bootstrap=${INPUT_ignition_bootstrap}
    TF_VAR_lb_cloudscale_api_secret=${INPUT_cloudscale_token_floaty}
    TF_VAR_control_vshn_net_token=${INPUT_control_vshn_api_token}
    GIT_AUTHOR_NAME=$(git config --global user.name)
    GIT_AUTHOR_EMAIL=$(git config --global user.email)
    HIERADATA_REPO_TOKEN=${INPUT_hieradata_repo_token}
    EOF

    tf_image=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.image" \
      dependencies/openshift4-terraform/class/defaults.yml)
    tf_tag=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.tag" \
      dependencies/openshift4-terraform/class/defaults.yml)

    echo "Using Terraform image: ${tf_image}:${tf_tag}"

    base_dir=$(pwd)
    alias terraform='touch .terraformrc; docker run --rm -e REAL_UID=$(id -u) -e TF_CLI_CONFIG_FILE=/tf/.terraformrc --env-file ${base_dir}/terraform.env -w /tf -v $(pwd):/tf --ulimit memlock=-1 "${tf_image}:${tf_tag}" /tf/terraform.sh'

    gitlab_repository_url=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${INPUT_commodore_api_url}/clusters/${INPUT_commodore_cluster_id} | jq -r '.gitRepo.url' | sed 's|ssh://||; s|/|:|')
    gitlab_repository_name=${gitlab_repository_url##*/}
    gitlab_catalog_project_id=$(curl -sH "Authorization: Bearer ${INPUT_gitlab_api_token}" "https://git.vshn.net/api/v4/projects?simple=true&search=${gitlab_repository_name/.git}" | jq -r ".[] | select(.ssh_url_to_repo == \"${gitlab_repository_url}\") | .id")
    gitlab_state_url="https://git.vshn.net/api/v4/projects/${gitlab_catalog_project_id}/terraform/state/cluster"

    pushd catalog/manifests/openshift4-terraform/

    terraform init \
      "-backend-config=address=${gitlab_state_url}" \
      "-backend-config=lock_address=${gitlab_state_url}/lock" \
      "-backend-config=unlock_address=${gitlab_state_url}/lock" \
      "-backend-config=username=${INPUT_gitlab_user_name}" \
      "-backend-config=password=${INPUT_gitlab_api_token}" \
      "-backend-config=lock_method=POST" \
      "-backend-config=unlock_method=DELETE" \
      "-backend-config=retry_wait_min=5"

    rm override.tf
    terraform apply --auto-approve

    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    echo "@                                                                                       @"
    echo "@  Please review and merge the LB hieradata MR listed in Terraform output hieradata_mr. @"
    echo "@                                                                                       @"
    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    while (GITLAB_HOST=git.vshn.net GITLAB_TOKEN="${INPUT_gitlab_api_token}" glab mr list -R=appuio/appuio_hieradata | grep "${INPUT_commodore_cluster_id}")
    do
      sleep 10
    done
    echo PR merged, waiting for CI to finish...
    sleep 10
    while (GITLAB_HOST=git.vshn.net GITLAB_TOKEN="${INPUT_gitlab_api_token}" glab mr list -R=appuio/appuio_hieradata | grep "running")
    do
      sleep 10
    done

    ssh "${INPUT_lb_fqdn_1}" sudo puppetctl run
    ssh "${INPUT_lb_fqdn_2}" sudo puppetctl run

    popd

- match: And I configure initial deployments
  description: |-
    This step configures some deployments that require manual changes after
    cluster bootstrap, such as reverting the Cilium patch from earlier, enabling
    proxy protocol on the Ingress controller, and scheduling the ingress
    controller on the infrastructure nodes.
  inputs:
  - name: commodore_cluster_id
  - name: commodore_api_url
  - name: kubeconfig_path
  run: |
    set -euo pipefail
    export COMMODORE_API_URL="${INPUT_commodore_api_url}"
    installer_dir="$(pwd)/target"
    export KUBECONFIG="${INPUT_kubeconfig_path}"

    echo '# Patching cilium ... #'
    kubectl patch network.operator cluster --type=merge \
      -p '{"spec":{"deployKubeProxy":false}}'
    kubectl -n cilium replace -f catalog/manifests/cilium/olm/cluster-network-07-cilium-ciliumconfig.yaml
    while ! kubectl -n cilium get cm cilium-config -oyaml | grep 'kube-proxy-replacement: "true"' &>/dev/null; do
      echo -n "."
      sleep 2
    done && echo -e "\nCilium config updated"
    kubectl -n cilium rollout restart ds/cilium
    echo '# Patched cilium. #'
    echo
    echo '# Enabling proxy protocol ... #'
    kubectl -n openshift-ingress-operator patch ingresscontroller default --type=json \
      -p '[{
        "op":"replace",
        "path":"/spec/endpointPublishingStrategy",
        "value": {"type": "HostNetwork", "hostNetwork": {"protocol": "PROXY"}}
      }]'
    echo '# Enabled proxy protocol. #'
    echo

    distribution=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${COMMODORE_API_URL}/clusters/${INPUT_commodore_cluster_id} | jq -r .facts.distribution)
    if [[ distribution != "oke" ]]
    then
      echo '# Scheduling ingress controller on infra nodes ... #'
      kubectl -n openshift-ingress-operator patch ingresscontroller default --type=json \
        -p '[{
          "op":"replace",
          "path":"/spec/nodePlacement",
          "value":{"nodeSelector":{"matchLabels":{"node-role.kubernetes.io/infra":""}}}
        }]'
      echo '# Scheduled ingress controller on infra nodes. #'
      echo
    fi

    echo '# Removing temporary cert-manager tolerations ... #'
    kubectl -n syn-cert-manager patch --type=json \
      $(kubectl -n syn-cert-manager get deploy -oname) \
      -p '[{"op":"remove","path":"/spec/template/spec/tolerations"}]'
    echo '# Removed temporary cert-manager tolerations. #'

- match: And I wait for installation to complete
  description: |-
    This step waits for OpenShift installation to complete successfully.
  run: |
    set -euo pipefail
    installer_dir="$(pwd)/target"
    openshift-install --dir "${installer_dir}" \
      wait-for install-complete --log-level debug
