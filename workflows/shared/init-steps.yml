steps:
- match: And I have the `openshift-install` binary for version "(?P<ocp_version>[^"]+)"
  description: |-
    This step checks if the `openshift-install` binary for the specified OpenShift version is available in your PATH.

    If not found, it provides instructions on how to download it.
  run: |
    set -euo pipefail

    if command -v openshift-install >/dev/null 2>&1; then
      INSTALLED_VERSION=$(openshift-install version | grep 'openshift-install' | awk '{print $2}' | sed 's/^v//' | sed -E 's/\.[0-9]{1,2}$//')
      if [ "$INSTALLED_VERSION" = "$MATCH_ocp_version" ]; then
        echo "✅ openshift-install version ${MATCH_ocp_version}.XX is installed."
        exit 0
      else
        echo "❌ openshift-install version $INSTALLED_VERSION is installed, but version $MATCH_ocp_version is required. Please download the openshift-install binary for version $MATCH_ocp_version from https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-${MATCH_ocp_version}/ and add it to your PATH."
        exit 1
      fi
    else
      echo "❌ openshift-install binary not found in PATH. Please download the openshift-install binary for version $MATCH_ocp_version"
      echo "from https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable-${MATCH_ocp_version}/ and add it to your PATH."
      exit 1
    fi
- match: And a lieutenant cluster
  description: |-
    This step retrieves the Commodore tenant ID associated with the given lieutenant cluster ID.

    Use https://api.syn.vshn.net as the Commodore API URL for production clusters.
    You might use the WebUI at https://control.vshn.net/syn/lieutenantapiendpoints to create and manage your clusters.

    For customer clusters ensure the following facts are set:
    * sales_order: Name of the sales order to which the cluster is billed, such as S10000
    * service_level: Name of the service level agreement for this cluster, such as guaranteed-availability
    * access_policy: Access-Policy of the cluster, such as regular or swissonly
    * release_channel: Name of the syn component release channel to use, such as stable
    * maintenance_window: Pick the appropriate upgrade schedule, such as monday-1400 for test clusters, tuesday-1000 for prod or custom to not (yet) enable maintenance
    * cilium_addons: Comma-separated list of cilium addons the customer gets billed for, such as advanced_networking or tetragon. Set to NONE if no addons should be billed.

    This step checks that you have access to the Commodore API and the cluster ID is valid.
  inputs:
  - name: commodore_api_url
    description: |-
      URL of the Commodore API to use for retrieving cluster information.

      Use https://api.syn.vshn.net as the Commodore API URL for production clusters.
      Use https://api-int.syn.vshn.net for test clusters.

      You might use the WebUI at https://control.vshn.net/syn/lieutenantapiendpoints to create and manage your clusters.

  - name: commodore_cluster_id
    description: |-
      Project Syn cluster ID for the cluster to be set up.

      In the form of c-example-infra-prod1.

      You might use the WebUI at https://control.vshn.net/syn/lieutenantapiendpoints to create and manage your clusters.
  outputs:
  - name: commodore_tenant_id
  - name: csp_region
  run: |
    set -euo pipefail
    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    echo "Retrieving Commodore tenant ID for cluster ID '$INPUT_commodore_cluster_id' from API at '$INPUT_commodore_api_url'..."
    tenant_id=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${COMMODORE_API_URL}/clusters/${INPUT_commodore_cluster_id} | jq -r .tenant)
    if echo "$tenant_id" | grep 't-' >/dev/null 2>&1 ; then { echo "✅ Retrieved tenant ID '$tenant_id' for cluster ID '$INPUT_commodore_cluster_id'."; } else { echo "❌ Failed to retrieve valid tenant ID for cluster ID '$INPUT_commodore_cluster_id'. Got '$tenant_id'. Please check your Commodore API access and cluster ID."; exit 1; } ; fi
    env -i "commodore_tenant_id=$tenant_id" >> "$OUTPUT"

    region=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${COMMODORE_API_URL}/clusters/${INPUT_commodore_cluster_id} | jq -r .facts.region)
    if test -z "$region" && test "$region" != "null" ; then { echo "❌ Failed to retrieve CSP region for cluster ID '$INPUT_commodore_cluster_id'."; exit 1; } ; else { echo "✅ Retrieved CSP region '$region' for cluster ID '$INPUT_commodore_cluster_id'."; } ; fi
    env -i "csp_region=$region" >> "$OUTPUT"
- match: And a Keycloak service
  description: |-
    In this step, you have to create a Keycloak service for the new cluster
    via the VSHN Control Web UI at https://control.vshn.net/vshn/services/_create 
  inputs:
  - name: commodore_cluster_id
  run: |
    echo '#########################################################'
    echo '#                                                       #'
    echo "#  Please create a Keycloak service with the cluster's  #"
    echo '#  ID as Service Name via the VSHN Control Web UI.      #'
    echo '#                                                       #'
    echo '#########################################################'
    echo
    echo "The name and ID of the service should be ${INPUT_commodore_cluster_id}."
    echo "You can go to https://control.vshn.net/vshn/services/_create"
    sleep 2


- match: And a personal VSHN GitLab access token
  description: |-
    This step ensures that you have provided a personal access token for VSHN GitLab.

    Create the token at https://git.vshn.net/-/user_settings/personal_access_tokens with the "api" scope.

    This step currently does not validate the token's scope.
  inputs:
  - name: gitlab_api_token
    description: |-
      Personal access token for VSHN GitLab with the "api" scope.

      Create the token at https://git.vshn.net/-/user_settings/personal_access_tokens with the "api" scope.
  outputs:
  - name: gitlab_user_name
    description: |-
      Your GitLab user name.
  run: |
    set -euo pipefail
    user="$( curl -sH "Authorization: Bearer ${INPUT_gitlab_api_token}" "https://git.vshn.net/api/v4/user" | jq -r .username )"
    if [[ "$user" == "null" ]]
    then
      echo "Error validating GitLab token. Are you sure it is valid?"
      exit 1
    fi
    env -i "gitlab_user_name=$user" >> "$OUTPUT"
    echo "Token is valid."
- match: And a control.vshn.net Servers API token
  description: |-
    This step ensures that you have provided an API token for control.vshn.net Servers API.

    Create the token at https://control.vshn.net/tokens/_create/servers and ensure your IP is allowlisted.
  inputs:
  - name: control_vshn_api_token
    description: |-
      API token for control.vshn.net Servers API.

      Used to create the puppet based LBs.

      Be extra careful with the IP allowlist.
  run: |
    set -euo pipefail

    AUTH="X-AccessToken: ${INPUT_control_vshn_api_token}"

    code="$( curl -H"$AUTH" https://control.vshn.net/api/servers/1/appuio/ -o /dev/null -w"%{http_code}" )"

    if [[ "$code" != 200 ]]
    then
      echo "ERROR: could not access Server API (Status $code)"
      echo "Please ensure your token is valid and your IP is on the allowlist."
      exit 1
    fi
- match: And basic cluster information
  description: |-
    This step collects two essential pieces of information required for cluster setup: the base domain and the Red Hat pull secret.

    See https://kb.vshn.ch/oc4/explanations/dns_scheme.html for more information about the base domain.
    Get a pull secret from https://cloud.redhat.com/openshift/install/pull-secret.
  inputs:
  - name: base_domain
    description: |-
      The base domain for the cluster without the cluster ID prefix and the last dot.

      Example: `appuio-beta.ch`

      See https://kb.vshn.ch/oc4/explanations/dns_scheme.html for more information about the base domain.
  - name: redhat_pull_secret
    description: |-
      Red Hat pull secret for accessing Red Hat container images.

      Get a pull secret from https://cloud.redhat.com/openshift/install/pull-secret.
- match: And I check the cluster domain
  description: |-
    Please verify that the base domain generated is correct for your setup.
  inputs:
  - name: commodore_cluster_id
  - name: base_domain
  outputs:
  - name: cluster_domain
  run: |
    set -euo pipefail

    cluster_domain="${INPUT_commodore_cluster_id}.${INPUT_base_domain}"
    echo "Cluster domain is set to '$cluster_domain'"
    echo "cluster_domain=$cluster_domain" >> "$OUTPUT"

- match: And I configure Terraform for team "(?P<team_name>[^"]+)"
  description: |-
    This step configures Terraform the Commodore rendered terraform configuration.
  inputs:
  - name: commodore_api_url
  - name: commodore_cluster_id
  - name: commodore_tenant_id
  - name: ssh_public_key_path
  - name: hieradata_repo_user
  - name: base_domain
  - name: image_major
  - name: image_minor
  run: |
    set -euo pipefail

    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    installer_dir="$(pwd)/target"

    pushd "inventory/classes/${INPUT_commodore_tenant_id}/"

    yq eval -i '.classes += ["global.distribution.openshift4.no-opsgenie"]' ${INPUT_commodore_cluster_id}.yml;
    yq eval -i '.classes = (.classes | unique)' ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift.infraID = \"$(jq -r .infraID "${installer_dir}/metadata.json")\"" \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift.clusterID = \"$(jq -r .clusterID "${installer_dir}/metadata.json")\"" \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i 'del(.parameters.cilium.olm.generate_olm_deployment)' \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift.ssh_key = \"$(cat ${INPUT_ssh_public_key_path})\"" \
      ${INPUT_commodore_cluster_id}.yml

    ca_cert=$(jq -r '.ignition.security.tls.certificateAuthorities[0].source' \
      "${installer_dir}/master.ign" | \
      awk -F ',' '{ print $2 }' | \
      base64 --decode)

    yq eval -i ".parameters.openshift4_terraform.terraform_variables.base_domain = \"${INPUT_base_domain}\"" \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift4_terraform.terraform_variables.ignition_ca = \"${ca_cert}\"" \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift4_terraform.terraform_variables.ssh_keys = [\"$(cat ${INPUT_ssh_public_key_path})\"]" \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift4_terraform.terraform_variables.allocate_router_vip_for_lb_controller = true" \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift4_terraform.terraform_variables.team = \"${MATCH_team_name}\"" \
      ${INPUT_commodore_cluster_id}.yml

    yq eval -i ".parameters.openshift4_terraform.terraform_variables.hieradata_repo_user = \"${INPUT_hieradata_repo_user}\"" \
      ${INPUT_commodore_cluster_id}.yml

    git commit -a -m "Setup cluster ${INPUT_commodore_cluster_id}"
    git push

    popd

    commodore catalog compile ${INPUT_commodore_cluster_id} --push \
      --dynamic-fact kubernetesVersion.major=1 \
      --dynamic-fact kubernetesVersion.minor="$((INPUT_image_minor+13))" \
      --dynamic-fact openshiftVersion.Major=${INPUT_image_major} \
      --dynamic-fact openshiftVersion.Minor=${INPUT_image_minor}

- match: And I wait for bootstrap to complete
  description: |-
    This step waits for OpenShift bootstrap to complete successfully.
  run: |
    set -euo pipefail
    installer_dir="$(pwd)/target"
    openshift-install --dir "${installer_dir}" \
      wait-for bootstrap-complete --log-level debug

- match: And I wait for installation to complete
  description: |-
    This step waits for OpenShift installation to complete successfully.
  run: |
    set -euo pipefail
    installer_dir="$(pwd)/target"
    openshift-install --dir "${installer_dir}" \
      wait-for install-complete --log-level debug
