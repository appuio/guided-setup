steps:
- match: Then I save the loadbalancer metadata
  description: |-
    This step gathers metadata on the LoadBalancer instances (such as their icinga zone and backup server), such that they can be properly decommissioned down the line.
  inputs:
  - name: cloudscale_token
  - name: cloudscale_token_floaty
  - name: control_vshn_api_token
  - name: hieradata_repo_token
  - name: gitlab_user_name
  - name: gitlab_api_token
  - name: commodore_cluster_id
  - name: commodore_api_url
  outputs:
  - name: lb_fqdn_1
  - name: lb_fqdn_2
  - name: lb_backup_1
  - name: lb_backup_2
  - name: lb_icinga_1
  - name: lb_icinga_2
  run: |
    set -euo pipefail
    echo "TODO"
    exit 1

    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    cat <<EOF > ./terraform.env
    CLOUDSCALE_API_TOKEN=${INPUT_cloudscale_token}
    TF_VAR_lb_cloudscale_api_secret=${INPUT_cloudscale_token_floaty}
    TF_VAR_control_vshn_net_token=${INPUT_control_vshn_api_token}
    GIT_AUTHOR_NAME=$(git config --global user.name)
    GIT_AUTHOR_EMAIL=$(git config --global user.email)
    HIERADATA_REPO_TOKEN=${INPUT_hieradata_repo_token}
    EOF

    tf_image=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.image" \
      dependencies/openshift4-terraform/class/defaults.yml)
    tf_tag=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.tag" \
      dependencies/openshift4-terraform/class/defaults.yml)

    echo "Using Terraform image: ${tf_image}:${tf_tag}"

    base_dir=$(pwd)
    alias terraform='touch .terraformrc; docker run --rm -e REAL_UID=$(id -u) -e TF_CLI_CONFIG_FILE=/tf/.terraformrc --env-file ${base_dir}/terraform.env -w /tf -v $(pwd):/tf --ulimit memlock=-1 "${tf_image}:${tf_tag}" /tf/terraform.sh'

    gitlab_repository_url=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${INPUT_commodore_api_url}/clusters/${INPUT_commodore_cluster_id} | jq -r '.gitRepo.url' | sed 's|ssh://||; s|/|:|')
    gitlab_repository_name=${gitlab_repository_url##*/}
    gitlab_catalog_project_id=$(curl -sH "Authorization: Bearer ${INPUT_gitlab_api_token}" "https://git.vshn.net/api/v4/projects?simple=true&search=${gitlab_repository_name/.git}" | jq -r ".[] | select(.ssh_url_to_repo == \"${gitlab_repository_url}\") | .id")
    gitlab_state_url="https://git.vshn.net/api/v4/projects/${gitlab_catalog_project_id}/terraform/state/cluster"

    pushd catalog/manifests/openshift4-terraform/

    terraform init \
      "-backend-config=address=${gitlab_state_url}" \
      "-backend-config=lock_address=${gitlab_state_url}/lock" \
      "-backend-config=unlock_address=${gitlab_state_url}/lock" \
      "-backend-config=username=${INPUT_gitlab_user_name}" \
      "-backend-config=password=${INPUT_gitlab_api_token}" \
      "-backend-config=lock_method=POST" \
      "-backend-config=unlock_method=DELETE" \
      "-backend-config=retry_wait_min=5"

    cat > override.tf <<EOF
    module "cluster" {
      bootstrap_count          = 0
      master_count             = 0
      infra_count              = 0
      worker_count             = 0
      additional_worker_groups = {}
    }
    EOF

    lb1=$(terraform state show "module.cluster.module.lb.cloudscale_server.lb[0]" | grep fqdn | awk '{print $2}' | tr -d ' "\r\n')
    lb2=$(terraform state show "module.cluster.module.lb.cloudscale_server.lb[1]" | grep fqdn | awk '{print $2}' | tr -d ' "\r\n')

    backup1=$(ssh "$lb1" "sudo grep 'server =' /etc/burp/burp.conf  | awk '{ print \$3 }'" )
    backup2=$(ssh "$lb2" "sudo grep 'server =' /etc/burp/burp.conf  | awk '{ print \$3 }'" )

    icinga1=$(ssh "$lb1" "sudo grep 'ParentZone' /etc/icinga2/constants.conf | awk '{  print substr(\$4, 2, length(\$4)-2) }'" )
    icinga2=$(ssh "$lb2" "sudo grep 'ParentZone' /etc/icinga2/constants.conf | awk '{  print substr(\$4, 2, length(\$4)-2) }'" )

    env -i "lb_fqdn_1=$lb1" >> "$OUTPUT"
    env -i "lb_fqdn_2=$lb2" >> "$OUTPUT"
    env -i "lb_backup_1=$backup1" >> "$OUTPUT"
    env -i "lb_backup_2=$backup2" >> "$OUTPUT"
    env -i "lb_icinga_1=$icinga1" >> "$OUTPUT"
    env -i "lb_icinga_2=$icinga2" >> "$OUTPUT"
    popd

- match: And I decommission Terraform resources
  description: |-
    This step decommissions all Terraform resources for the cluster.
  inputs:
  - name: cloudscale_token
  - name: cloudscale_token_floaty
  - name: control_vshn_api_token
  - name: hieradata_repo_token
  - name: gitlab_user_name
  - name: gitlab_api_token
  - name: commodore_cluster_id
  - name: commodore_api_url
  run: |
    set -euo pipefail
    echo "TODO"
    exit 1

    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    cat <<EOF > ./terraform.env
    CLOUDSCALE_API_TOKEN=${INPUT_cloudscale_token}
    TF_VAR_lb_cloudscale_api_secret=${INPUT_cloudscale_token_floaty}
    TF_VAR_control_vshn_net_token=${INPUT_control_vshn_api_token}
    GIT_AUTHOR_NAME=$(git config --global user.name)
    GIT_AUTHOR_EMAIL=$(git config --global user.email)
    HIERADATA_REPO_TOKEN=${INPUT_hieradata_repo_token}
    EOF

    tf_image=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.image" \
      dependencies/openshift4-terraform/class/defaults.yml)
    tf_tag=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.tag" \
      dependencies/openshift4-terraform/class/defaults.yml)

    echo "Using Terraform image: ${tf_image}:${tf_tag}"

    base_dir=$(pwd)
    alias terraform='touch .terraformrc; docker run --rm -e REAL_UID=$(id -u) -e TF_CLI_CONFIG_FILE=/tf/.terraformrc --env-file ${base_dir}/terraform.env -w /tf -v $(pwd):/tf --ulimit memlock=-1 "${tf_image}:${tf_tag}" /tf/terraform.sh'

    gitlab_repository_url=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${INPUT_commodore_api_url}/clusters/${INPUT_commodore_cluster_id} | jq -r '.gitRepo.url' | sed 's|ssh://||; s|/|:|')
    gitlab_repository_name=${gitlab_repository_url##*/}
    gitlab_catalog_project_id=$(curl -sH "Authorization: Bearer ${INPUT_gitlab_api_token}" "https://git.vshn.net/api/v4/projects?simple=true&search=${gitlab_repository_name/.git}" | jq -r ".[] | select(.ssh_url_to_repo == \"${gitlab_repository_url}\") | .id")
    gitlab_state_url="https://git.vshn.net/api/v4/projects/${gitlab_catalog_project_id}/terraform/state/cluster"

    pushd catalog/manifests/openshift4-terraform/

    terraform init \
      "-backend-config=address=${gitlab_state_url}" \
      "-backend-config=lock_address=${gitlab_state_url}/lock" \
      "-backend-config=unlock_address=${gitlab_state_url}/lock" \
      "-backend-config=username=${INPUT_gitlab_user_name}" \
      "-backend-config=password=${INPUT_gitlab_api_token}" \
      "-backend-config=lock_method=POST" \
      "-backend-config=unlock_method=DELETE" \
      "-backend-config=retry_wait_min=5"

    terraform state rm "module.cluster.module.lb.module.hiera[0].gitfile_checkout.appuio_hieradata"

    # Suppress errors on the first run; it is expected to fail
    terraform destroy --auto-approve || true

    terraform destroy --auto-approve
    popd

- match: Then I delete Cloudscale server groups
  description: |-
    This step cleans up server groups configured for this cluster on Cloudscale.
  inputs:
  - name: cloudscale_token
  - name: commodore_cluster_id
  run: |
    set -euo pipefail
    echo "TODO"
    exit 1
    for server_group_uuid in $(curl -H"Authorization: Bearer ${INPUT_cloudscale_token}" \
      https://api.cloudscale.ch/v1/server-groups | \
      jq -r --arg cluster_id "${INPUT_commodore_cluster_id}" \
      '.[]|select(.name|startswith($cluster_id))|.uuid'); do
      curl -H"Authorization: Bearer ${INPUT_cloudscale_token}" \
        "https://api.cloudscale.ch/v1/server-groups/${server_group_uuid}" \
        -XDELETE
    done

- match: And I delete all S3 buckets
  description: |-
    This step deletes the cluster's associated S3 buckets from Cloudscale.
  inputs:
  - name: exoscale_key
  - name: exoscale_secret
  - name: commodore_cluster_id
  run: |
    set -euo pipefail
    export EXOSCALE_API_KEY="${INPUT_exoscale_key}"
    export EXOSCALE_API_SECRET="${INPUT_exoscale_secret}"

    # Bootstrap bucket
    exo storage rb -r -f "${INPUT_commodore_cluster_id}-bootstrap" || true
    # OpenShift Image Registry bucket
    exo storage rb -r -f "${INPUT_commodore_cluster_id}-image-registry" || true
    # OpenShift Loki logstore
    exo storage rb -r -f "${INPUT_commodore_cluster_id}-logstore" || true

- match: And I delete the cluster backup
  description: |-
    This step deletes the cluster's associated backup bucket from Cloudscale.
  inputs:
  - name: cloudscale_token
  - name: vault_address
  - name: commodore_cluster_id
  - name: commodore_api_url
  - name: backup_deletion_confirmation
    description: |-
      Really delete the cluster backup?

      Type "YES" to delete the cluster backup. With any other value, backup deletion gets skipped.

      Remember to confirm backup deletion according to the 4-eye principle:
      you and a colleague should agree that the backup needs to be deleted.
  run: |
    set -euo pipefail
    echo "TODO"
    exit 1

    if [[ ${INPUT_backup_deletion_confirmation} != "YES" ]]
    then
        echo '################################################################'
        echo '#                                                              #'
        echo '#  Skipping backup deletion because no confirmation was given  #'
        echo '#                                                              #'
        echo '################################################################'
        exit 0
    fi
    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    LIEUTENANT_AUTH="Authorization: Bearer $(commodore fetch-token)"
    REPO_URL="$(curl -sH "${LIEUTENANT_AUTH}" "${INPUT_commodore_api_url}/clusters/${INPUT_commodore_cluster_id}" | jq -r .gitRepo.url)"

    if [ -e catalog ]
    then
      rm -rf catalog
    fi

    mkdir catalog
    git archive --remote "${REPO_URL}" master | tar -xC catalog

    export VAULT_ADDR=${INPUT_vault_address}
    vault login -method=oidc

    # extract restic credentials from catalog and vault
    restic_repo=s3:$(yq -o=json 'select(.kind == "Schedule")| .spec.backend.s3 | .endpoint + "/" + .bucket' catalog/manifests/cluster-backup/10_object.yaml | tr -d '"')
    export RESTIC_REPOSITORY="$restic_repo"
    echo "$RESTIC_REPOSITORY"

    s3_ep=s3:$(yq -o=json 'select(.kind == "Schedule")| .spec.backend.s3 | .endpoint' catalog/manifests/cluster-backup/10_object.yaml | tr -d '"')
    export S3_ENDPOINT="$s3_ep"
    echo "$S3_ENDPOINT"

    PASSWORD_KEY="$(yq -o=json 'select(.kind == "Secret" and .metadata.name == "objects-backup-password") | .stringData.password' catalog/manifests/cluster-backup/10_object.yaml | cut -d: -f2)"
    restic_pw=$(vault kv get -format json "clusters/kv/${PASSWORD_KEY%/*}" | jq -r ".data.data.${PASSWORD_KEY##*/}")
    export RESTIC_PASSWORD="$restic_pw"

    ID_KEY="$(yq -o=json 'select(.kind == "Secret" and .metadata.name == "objects-backup-s3-credentials") | .stringData.username' catalog/manifests/cluster-backup/10_object.yaml | cut -d: -f2)"
    aws_id=$(vault kv get -format json "clusters/kv/${ID_KEY%/*}" | jq -r ".data.data.${ID_KEY##*/}")
    export AWS_ACCESS_KEY_ID="$aws_id"

    SECRET_KEY="$(yq -o=json 'select(.kind == "Secret" and .metadata.name == "objects-backup-s3-credentials") | .stringData.password' catalog/manifests/cluster-backup/10_object.yaml | cut -d: -f2)"
    aws_secret=$(vault kv get -format json "clusters/kv/${SECRET_KEY%/*}" | jq -r ".data.data.${SECRET_KEY##*/}")
    export AWS_SECRET_ACCESS_KEY="$aws_secret"

    # retrieve latest restic snapshots
    OBJECT_ID="$(restic snapshots -H syn-cluster-backup --latest 1 --json | jq -r .[-1].id)"
    ETCD_ID="$(restic snapshots -H syn-cluster-backup-etcd --latest 1 --json | jq -r .[-1].id)"

    mkdir -p backup-dump-${INPUT_commodore_cluster_id}

    if [[ "$OBJECT_ID" != "null" ]]
    then
      restic restore "$OBJECT_ID" --target backup-dump-${INPUT_commodore_cluster_id}
    fi
    if [[ "$ETCD_ID" != "null" ]]
    then
      restic restore "$ETCD_ID" --target backup-dump-${INPUT_commodore_cluster_id}
    fi

    # proceed with deleting the backup bucket
    # Use already exiting bucket user
    response=$(curl -sH "Authorization: Bearer ${INPUT_cloudscale_token}" \
      https://api.cloudscale.ch/v1/objects-users | \
      jq -e ".[] | select(.display_name == \"${INPUT_commodore_cluster_id}\")")

    # configure minio client to use the bucket
    mc alias set \
      "${INPUT_commodore_cluster_id}_backup" "${S3_ENDPOINT##s3:}" \
      "$(echo "$response" | jq -r '.keys[0].access_key')" \
      "$(echo "$response" | jq -r '.keys[0].secret_key')"

    # Delete bucket
    mc rb "${INPUT_commodore_cluster_id}_backup/${INPUT_commodore_cluster_id}-cluster-backup" --force

    # Delete object user
    curl -i -H "Authorization: Bearer ${INPUT_cloudscale_token}" -X DELETE "$(echo "$response" | jq -r '.href')"

- match: And I delete the cluster's API tokens
  description: |-
    This step deletes the cluster's associated Exoscale API tokens from Exoscale.
  inputs:
  - name: exoscale_key
  - name: exoscale_secret
  - name: commodore_cluster_id
  run: |
    set -euo pipefail
    export EXOSCALE_API_KEY="${INPUT_exoscale_key}"
    export EXOSCALE_API_SECRET="${INPUT_exoscale_secret}"

    # delete restricted api keys
    exo iam api-key delete -f "${INPUT_commodore_cluster_id}_ccm-exoscale" || true
    exo iam api-key delete -f "${INPUT_commodore_cluster_id}_csi-driver-exoscale" || true
    exo iam api-key delete -f "${INPUT_commodore_cluster_id}_floaty" || true
    exo iam api-key delete -f "${INPUT_commodore_cluster_id}_object_storage" || true

    # delete decommissioning api key
    exo iam api-key delete -f "${INPUT_commodore_cluster_id}" || true
