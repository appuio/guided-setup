steps:
- match: Given I have all prerequisites installed
  description: |-
    This step checks if all necessary prerequisites are installed on your system,
    including 'yq' (version 4 or higher, by Mike Farah) and 'oc' (OpenShift CLI).
  run: |
    set -euo pipefail
    echo "Checking prerequisites..."

    if which yq >/dev/null 2>&1 ; then { echo "✅ yq is installed."; } ; else { echo "❌ yq is not installed. Please install yq to proceed."; exit 1; } ; fi
    if yq --version | grep -E 'version v[4-9]\.' | grep 'mikefarah' >/dev/null 2>&1 ; then { echo "✅ yq by mikefarah version 4 or higher is installed."; } ; else { echo "❌ yq version 4 or higher is required. Please upgrade yq to proceed."; exit 1; } ; fi

    if which jq >/dev/null 2>&1 ; then { echo "✅ jq is installed."; } ; else { echo "❌ jq is not installed. Please install jq to proceed."; exit 1; } ; fi

    if which oc >/dev/null 2>&1 ; then { echo "✅ oc (OpenShift CLI) is installed."; } ; else { echo "❌ oc (OpenShift CLI) is not installed. Please install oc to proceed."; exit 1; } ; fi

    if which vault >/dev/null 2>&1 ; then { echo "✅ vault (HashiCorp Vault) is installed."; } ; else { echo "❌ vault (HashiCorp Vault) is not installed. Please install vault to proceed."; exit 1; } ; fi

    if which curl >/dev/null 2>&1 ; then { echo "✅ curl is installed."; } ; else { echo "❌ curl is not installed. Please install curl to proceed."; exit 1; } ; fi

    if which docker >/dev/null 2>&1 ; then { echo "✅ docker is installed."; } ; else { echo "❌ docker is not installed. Please install docker to proceed."; exit 1; } ; fi

    if which glab >/dev/null 2>&1 ; then { echo "✅ glab (GitLab CLI) is installed."; } ; else { echo "❌ glab (GitLab CLI) is not installed. Please install glab to proceed."; exit 1; } ; fi

    if which host >/dev/null 2>&1 ; then { echo "✅ host (DNS lookup utility) is installed."; } ; else { echo "❌ host (DNS lookup utility) is not installed. Please install host to proceed."; exit 1; } ; fi

    if which mc >/dev/null 2>&1 ; then { echo "✅ mc (MinIO Client) is installed."; } ; else { echo "❌ mc (MinIO Client) is not installed. Please install mc >= RELEASE.2024-01-18T07-03-39Z to proceed."; exit 1; } ; fi
    mc_version=$(mc --version | grep -Eo 'RELEASE[^ ]+')
    if echo "$mc_version" | grep -E 'RELEASE\.202[4-9]-' >/dev/null 2>&1 ; then { echo "✅ mc version ${mc_version} is sufficient."; } ; else { echo "❌ mc version ${mc_version} is insufficient. Please upgrade mc to >= RELEASE.2024-01-18T07-03-39Z to proceed."; exit 1; } ; fi

    if which aws >/dev/null 2>&1 ; then { echo "✅ aws (AWS CLI) is installed."; } ; else { echo "❌ aws (AWS CLI) is not installed. Please install aws to proceed. Our recommended installer is uv: 'uv tool install awscli'"; exit 1; } ; fi

    if which exo >/dev/null 2>&1 ; then { echo "✅ exo (Exoscale CLI) is installed."; } ; else { echo "❌ exo (Exoscale CLI) is not installed. Please install exo to proceed. See: https://community.exoscale.com/tools/command-line-interface/"; exit 1; } ; fi

    if which restic >/dev/null 2>&1 ; then { echo "✅ restic (Backup CLI) is installed."; } ; else { echo "❌ restic (Backup CLI) is not installed. Please install restic to proceed."; exit 1; } ; fi

    if which gzip >/dev/null 2>&1 ; then { echo "✅ gzip (file compression) is installed."; } ; else { echo "❌ gzip (file compression) is not installed. Please install gzip to proceed."; exit 1; } ; fi

    if which md5sum >/dev/null 2>&1 ; then { echo "✅ md5sum (file checksums) is installed."; } ; else { echo "❌ md5sum (file checksums) is not installed. Please install md5sum to proceed."; exit 1; } ; fi

    if which virt-edit >/dev/null 2>&1 ; then { echo "✅ virt-edit (VM image editing) is installed."; } ; else { echo "❌ virt-edit (VM image editing) is not installed. Please install virt-edit to proceed."; exit 1; } ; fi

    if which cpio >/dev/null 2>&1 ; then { echo "✅ cpio (file archiving) is installed."; } ; else { echo "❌ cpio (file archiving) is not installed. Please install cpio to proceed."; exit 1; } ; fi

    if which emergency-credentials-receive >/dev/null 2>&1 ; then { echo "✅ emergency-credentials-receive (Cluster emergency access helper) is installed."; } ; else { echo "❌ emergency-credentials-receive is not installed. Please install it from https://github.com/vshn/emergency-credentials-receive ."; exit 1; } ; fi

    echo "✅ All prerequisites are met."
- match: And Exoscale API tokens
  inputs:
  - name: exoscale_key
    description: |-
      Exoscale API Key name

      Used for setting up or tearing down the cluster with terraform
  - name: exoscale_secret
    description: |-
      Exoscale API secret

      Used for setting up or tearing down the cluster with terraform
  - name: csp_region
    description: |-
      Name of the exoscale zone in which to set up the cluster.

      All lowercase. For example: ch-dk-2
  outputs:
  - name: exoscale_s3_endpoint
  description: |-
    Create a new Exoscale API key in the correct project, with IAMv3 role "Owner".

    This step currently does not validate the token's access scope.
  run: |
    set -euo pipefail
    export EXOSCALE_API_KEY="${INPUT_exoscale_key}"
    export EXOSCALE_API_SECRET="${INPUT_exoscale_secret}"
    exo limits > /dev/null || { echo "Invalid token!" ; exit 1 ; }
    echo "Token is valid"
    env -i "exoscale_s3_endpoint=sos-${INPUT_csp_region}.exo.io" >> "$OUTPUT"
- match: Then I check the Exoscale resource quotas
  inputs:
  - name: exoscale_key
  - name: exoscale_secret
  description: |-
    Checks the quotas in your Exoscale org for DNS and instances.

    If the quotas are too low, you must manually increase them.
  run: |
    set -euo pipefail
    export EXOSCALE_API_KEY="${INPUT_exoscale_key}"
    export EXOSCALE_API_SECRET="${INPUT_exoscale_secret}"

    instances_used="$( exo limits -Ojson | jq '.[] | select(.resource == "Compute instances").used'  )"
    instances_max="$( exo limits -Ojson | jq '.[] | select(.resource == "Compute instances").max'  )"

    if (( instances_max - instances_used < 15 ))
    then
      echo "Instance quota is insufficient! Please increase the instance quota."
      echo "Visit https://portal.exoscale.com/u/$( exo x get-organization | jq -r .id )/organization/quotas "
      exit 1
    fi
    echo "Instance quota is sufficient"

    echo '###################################################################################'
    echo '#                                                                                 #'
    echo '#  Please check the DNS subscription and upgrade it if no DNS Domains are left.   #'
    echo '#                                                                                 #'
    echo '###################################################################################'
    echo
    echo "See https://portal.exoscale.com/u/$( exo x get-organization | jq -r .id )/dns"

- match: Then I create the necessary Exoscale IAM keys
  description: |-
    This step creates restricted API keys for object storage, CSI driver, and CCM.
  inputs:
  - name: exoscale_key
  - name: exoscale_secret
  - name: commodore_cluster_id
  outputs:
  - name: s3_key
  - name: s3_secret
  - name: csi_key
  - name: csi_secret
  - name: ccm_key
  - name: ccm_secret
  run: |
    set -euo pipefail
    export EXOSCALE_API_KEY="${INPUT_exoscale_key}"
    export EXOSCALE_API_SECRET="${INPUT_exoscale_secret}"

    echo "Create SOS IAM role, if it doesn't exist yet in the organization"
    sos_iam_role_id=$(exo iam role list -O json | \
      jq -r '.[] | select(.name=="sos-full-access") | .key')
    if [ -z "${sos_iam_role_id}" ]; then
    echo '{
      "default-service-strategy": "deny",
      "services": {
        "sos": {"type": "allow"}
      }
    }' | \
    exo iam role create sos-full-access \
      --description "Full access to object storage service" \
      --policy -
    fi
    echo "TODO: Check whether the key with that name already exists"
    # Create access key
    exoscale_s3_credentials=$(exo iam api-key create -O json \
      "${INPUT_commodore_cluster_id}_object_storage" sos-full-access)
    EXOSCALE_S3_ACCESSKEY=$(echo "${exoscale_s3_credentials}" | jq -r '.key')
    EXOSCALE_S3_SECRETKEY=$(echo "${exoscale_s3_credentials}" | jq -r '.secret')

    echo
    echo "Create Exoscale CSI driver Exoscale IAM role, if it doesn't exist yet in the organization"
    csidriver_role_id=$(exo iam role list -O json | \
      jq -r '.[] | select(.name=="csi-driver-exoscale") | .key')
    if [ -z "${csidriver_role_id}" ]; then
    cat << EOF | exo iam role create csi-driver-exoscale \
      --description "Exoscale CSI Driver: Access to storage operations and zone list" \
      --policy -
    {
      "default-service-strategy": "deny",
      "services": {
        "compute": {
          "type": "rules",
          "rules": [
            {
              "expression": "operation in ['list-zones', 'get-block-storage-volume', 'list-block-storage-volumes', 'create-block-storage-volume', 'delete-block-storage-volume', 'attach-block-storage-volume-to-instance', 'detach-block-storage-volume', 'update-block-storage-volume-labels', 'resize-block-storage-volume', 'get-block-storage-snapshot', 'list-block-storage-snapshots', 'create-block-storage-snapshot', 'delete-block-storage-snapshot']",
              "action": "allow"
            }
          ]
        }
      }
    }
    EOF
    fi
    echo "TODO: Check whether the key with that name already exists"
    # Create access key
    csi_credentials=$(exo iam api-key create -O json \
      "${INPUT_commodore_cluster_id}_csi-driver-exoscale" csi-driver-exoscale)
    CSI_ACCESSKEY=$(echo "${csi_credentials}" | jq -r '.key')
    CSI_SECRETKEY=$(echo "${csi_credentials}" | jq -r '.secret')

    echo
    echo "Create Exoscale CCM Exoscale IAM role, if it doesn't exist yet in the organization"
    ccm_role_id=$(exo iam role list -O json | \
      jq -r '.[] | select(.name=="ccm-exoscale") | .key')
    if [ -z "${ccm_role_id}" ]; then
    cat <<EOF | exo iam role create ccm-exoscale \
      --description "Exoscale CCM: Allow managing NLBs and reading instances/instance pools" \
      --policy -
    {
      "default-service-strategy": "deny",
      "services": {
        "compute": {
          "type": "rules",
          "rules": [
            {
              "expression": "operation in ['add-service-to-load-balancer', 'create-load-balancer', 'delete-load-balancer', 'delete-load-balancer-service', 'get-load-balancer', 'get-load-balancer-service', 'get-operation', 'list-load-balancers', 'reset-load-balancer-field', 'reset-load-balancer-service-field', 'update-load-balancer', 'update-load-balancer-service']",
              "action": "allow"
            },
            {
              "expression": "operation in ['get-instance', 'get-instance-pool', 'get-instance-type', 'list-instances', 'list-instance-pools', 'list-zones']",
              "action": "allow"
            }
          ]
        }
      }
    }
    EOF
    fi
    echo "TODO: Check whether the key with that name already exists"
    # Create access key
    ccm_credentials=$(exo iam api-key create -O json \
      "${INPUT_commodore_cluster_id}_ccm-exoscale" ccm-exoscale)
    CCM_ACCESSKEY=$(echo "${ccm_credentials}" | jq -r '.key')
    CCM_SECRETKEY=$(echo "${ccm_credentials}" | jq -r '.secret')

    env -i "s3_key=${EXOSCALE_S3_ACCESSKEY}" >> "$OUTPUT"
    env -i "s3_secret=${EXOSCALE_S3_SECRETKEY}" >> "$OUTPUT"
    env -i "csi_key=${CSI_ACCESSKEY}" >> "$OUTPUT"
    env -i "csi_secret=${CSI_SECRETKEY}" >> "$OUTPUT"
    env -i "ccm_key=${CCM_ACCESSKEY}" >> "$OUTPUT"
    env -i "ccm_secret=${CCM_SECRETKEY}" >> "$OUTPUT"

- match: Then I set secrets in Vault
  description: |-
    This step stores the collected secrets and tokens in the ProjectSyn Vault.
  inputs:
  - name: vault_address
    description: |-
      Address of the Vault server associated with the Lieutenant API to store cluster secrets.

      https://vault-prod.syn.vshn.net/ for production clusters.
  - name: commodore_cluster_id
  - name: commodore_tenant_id
  - name: s3_key
  - name: s3_secret
  - name: csi_key
  - name: csi_secret
  - name: ccm_key
  - name: ccm_secret

  outputs:
  - name: hieradata_repo_user
  - name: hieradata_repo_token
  run: |
    set -euo pipefail

    export VAULT_ADDR=${INPUT_vault_address}
    vault login -method=oidc

    # Set the Exoscale object storage API key
    vault kv put clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/exoscale/storage_iam \
      s3_access_key=${INPUT_s3_key} \
      s3_secret_key=${INPUT_s3_secret}

    # Generate an HTTP secret for the registry
    vault kv put clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/registry \
      httpSecret="$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 128)"

    # Generate a master password for K8up backups
    vault kv put clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/global-backup \
      password="$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)"

    # Generate a password for the cluster object backups
    vault kv put clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/cluster-backup \
      password="$(LC_ALL=C tr -cd "A-Za-z0-9" </dev/urandom | head -c 32)"

    # Set the CSI Driver Exoscale Credentials
    vault kv put clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/exoscale/csi_driver \
      access_key=${INPUT_csi_key} \
      secret_key=${INPUT_csi_secret}

    # Set the CCM Exoscale Credentials
    vault kv put clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/exoscale/ccm \
      access_key=${INPUT_ccm_key} \
      secret_key=${INPUT_ccm_secret}


    hieradata_repo_secret=$(vault kv get \
      -format=json "clusters/kv/lbaas/hieradata_repo_token" | jq '.data.data')
    env -i "hieradata_repo_user=$(echo "${hieradata_repo_secret}" | jq -r '.user')" >> "$OUTPUT"
    env -i "hieradata_repo_token=$(echo "${hieradata_repo_secret}" | jq -r '.token')" >> "$OUTPUT"

- match: And I prepare the cluster repository
  description: |-
    This step prepares the local cluster repository by cloning the Commodore hieradata repository
    and setting up the necessary configuration for the specified cluster.
  inputs:
  - name: commodore_api_url
  - name: commodore_cluster_id
  - name: commodore_tenant_id
  - name: cluster_domain
  - name: image_major
  - name: image_minor
  run: |
    set -euo pipefail

    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    rm -rf inventory/classes/
    mkdir -p inventory/classes/
    git clone "$(curl -sH"Authorization: Bearer $(commodore fetch-token)" "${INPUT_commodore_api_url}/tenants/${INPUT_commodore_tenant_id}" | jq -r '.gitRepo.url')" inventory/classes/${INPUT_commodore_tenant_id}

    pushd "inventory/classes/${INPUT_commodore_tenant_id}/"

    yq eval -i ".parameters.openshift.baseDomain = \"${INPUT_cluster_domain}\"" \
      ${INPUT_commodore_cluster_id}.yml

    git diff --exit-code --quiet || git commit -a -m "Configure cluster domain for ${INPUT_commodore_cluster_id}"

    if ls openshift4.y*ml 1>/dev/null 2>&1; then
      yq eval -i '.classes += ".openshift4"' ${INPUT_commodore_cluster_id}.yml;
      git diff --exit-code --quiet || git commit -a -m "Include openshift4 class for ${INPUT_commodore_cluster_id}"
    fi

    yq eval -i '.applications += ["exoscale-cloud-controller-manager"]' ${INPUT_commodore_cluster_id}.yml
    yq eval -i '.applications = (.applications | unique)' ${INPUT_commodore_cluster_id}.yml
    git diff --exit-code --quiet || git commit -a -m "Deploy Exoscale cloud-controller-manager on ${INPUT_commodore_cluster_id}"


    yq eval -i '.applications += ["cilium"]' ${INPUT_commodore_cluster_id}.yml
    yq eval -i '.applications = (.applications | unique)' ${INPUT_commodore_cluster_id}.yml

    yq eval -i '.parameters.openshift.infraID = "TO_BE_DEFINED"' ${INPUT_commodore_cluster_id}.yml
    yq eval -i '.parameters.openshift.clusterID = "TO_BE_DEFINED"' ${INPUT_commodore_cluster_id}.yml

    yq eval -i '.parameters.cilium.olm.generate_olm_deployment = true' ${INPUT_commodore_cluster_id}.yml

    git diff --exit-code --quiet || git commit -a -m "Add Cilium addon to ${INPUT_commodore_cluster_id}"

    git push

    popd

    commodore catalog compile ${INPUT_commodore_cluster_id} --push \
      --dynamic-fact kubernetesVersion.major=1 \
      --dynamic-fact kubernetesVersion.minor="$((INPUT_image_minor+13))" \
      --dynamic-fact openshiftVersion.Major=${INPUT_image_major} \
      --dynamic-fact openshiftVersion.Minor=${INPUT_image_minor}

- match: Then I configure the OpenShift installer
  description: |-
    This step configures the OpenShift installer for the Exoscale cluster by generating
    the necessary installation files using Commodore.
  inputs:
  - name: exoscale_key
  - name: exoscale_secret
  - name: commodore_cluster_id
  - name: commodore_tenant_id
  - name: base_domain
  - name: cluster_domain
  - name: vault_address
  - name: redhat_pull_secret
  - name: ccm_key
  - name: ccm_secret
  outputs:
  - name: ignition_bootstrap
  - name: ssh_public_key_path
  run: |
    set -euo pipefail
    export EXOSCALE_API_KEY="${INPUT_exoscale_key}"
    export EXOSCALE_API_SECRET="${INPUT_exoscale_secret}"

    export VAULT_ADDR="${INPUT_vault_address}"
    vault login -method=oidc

    ssh_private_key="$(pwd)/ssh_${INPUT_commodore_cluster_id}"
    ssh_public_key="${ssh_private_key}.pub"

    env -i "ssh_public_key_path=$ssh_public_key" >> "$OUTPUT"

    if vault kv get -format=json clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/exoscale/ssh >/dev/null 2>&1; then
      echo "SSH keypair for cluster ${INPUT_commodore_cluster_id} already exists in Vault, skipping generation."

      vault kv get -format=json clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/exoscale/ssh | \
        jq -r '.data.data.private_key|@base64d' > "${ssh_private_key}"

      chmod 600 "${ssh_private_key}"
      ssh-keygen -f "${ssh_private_key}" -y > "${ssh_public_key}"

    else
      echo "Generating new SSH keypair for cluster ${INPUT_commodore_cluster_id}."

      ssh-keygen -C "vault@${INPUT_commodore_cluster_id}" -t ed25519 -f "$ssh_private_key" -N ''

      base64_no_wrap='base64'
      if [[ "$OSTYPE" == "linux"* ]]; then
        base64_no_wrap='base64 --wrap 0'
      fi

      vault kv put clusters/kv/${INPUT_commodore_tenant_id}/${INPUT_commodore_cluster_id}/exoscale/ssh \
        private_key="$(cat "$ssh_private_key" | eval "$base64_no_wrap")"
    fi

    echo Adding SSH private key to ssh-agent...
    echo You might need to start the ssh-agent first using: eval "\$(ssh-agent)"
    echo ssh-add "$ssh_private_key"
    ssh-add "$ssh_private_key"

    installer_dir="$(pwd)/target"
    rm -rf "${installer_dir}"
    mkdir -p "${installer_dir}"

    cat > "${installer_dir}/install-config.yaml" <<EOF
    apiVersion: v1
    metadata:
      name: ${INPUT_commodore_cluster_id} 
    baseDomain: ${INPUT_base_domain} 
    platform:
      external:
        platformName: exoscale
        cloudControllerManager: External
    networking: 
      networkType: Cilium
    pullSecret: |
      ${INPUT_redhat_pull_secret}
    sshKey: "$(cat "$ssh_public_key")"
    EOF

    echo Running OpenShift installer to create manifests...
    openshift-install --dir "${installer_dir}" create manifests

    echo Copying machineconfigs...
    machineconfigs=catalog/manifests/openshift4-nodes/10_machineconfigs.yaml
    if [ -f $machineconfigs ];  then
      yq --no-doc -s \
        "\"${installer_dir}/openshift/99x_openshift-machineconfig_\" + .metadata.name" \
        $machineconfigs
    fi

    echo Copying Exoscale CCM manifests...
    for f in catalog/manifests/exoscale-cloud-controller-manager/manager/*; do
      cp "$f" "${installer_dir}/manifests/exoscale_ccm_$(basename "$f")"
    done
    
    yq -i e ".stringData.api-key=\"${INPUT_ccm_key}\",.stringData.api-secret=\"${INPUT_ccm_secret}\"" \
      "${installer_dir}/manifests/exoscale_ccm_01_secret.yaml"

    echo Copying Cilium OLM manifests...
    for f in catalog/manifests/cilium/olm/[a-z]*; do
      cp "$f" "${installer_dir}/manifests/cilium_$(basename "$f")"
    done

    # shellcheck disable=2016
    # We don't want the shell to execute network.operator.openshift.io as a
    # command, so we need single quotes here.
    echo 'Generating initial `network.operator.openshift.io` resource...'
    yq '{
    "apiVersion": "operator.openshift.io/v1",
    "kind": "Network",
    "metadata": {
      "name": "cluster"
    },
    "spec": {
      "deployKubeProxy": false,
      "clusterNetwork": .spec.clusterNetwork,
      "externalIP": {
        "policy": {}
      },
      "networkType": "Cilium",
      "serviceNetwork": .spec.serviceNetwork
    }}' "${installer_dir}/manifests/cluster-network-02-config.yml" \
    > "${installer_dir}/manifests/cilium_cluster-network-operator.yaml"

    gen_cluster_domain=$(yq e '.spec.baseDomain' \
      "${installer_dir}/manifests/cluster-dns-02-config.yml")
    if [ "$gen_cluster_domain" != "$INPUT_cluster_domain" ]; then
      echo -e "\033[0;31mGenerated cluster domain doesn't match expected cluster domain: Got '$gen_cluster_domain', want '$INPUT_cluster_domain'\033[0;0m"
      exit 1
    else
      echo -e "\033[0;32mGenerated cluster domain matches expected cluster domain.\033[0;0m"
    fi

    echo Running OpenShift installer to create ignition configs...
    openshift-install --dir "${installer_dir}" \
      create ignition-configs

    exo storage upload "${installer_dir}/bootstrap.ign" "sos://${INPUT_commodore_cluster_id}-bootstrap" --acl public-read

    echo "✅ OpenShift installer configured successfully."

- match: Then I provision the loadbalancers
  description: |-
    This step provisions the load balancers for the Cloudscale OpenShift cluster using Terraform.
  inputs:
  - name: cloudscale_token
  - name: cloudscale_token_floaty
  - name: control_vshn_api_token
  - name: ignition_bootstrap
  - name: hieradata_repo_token
  - name: gitlab_user_name
  - name: gitlab_api_token
  - name: commodore_cluster_id
  - name: commodore_api_url
  - name: cluster_domain
  outputs:
  - name: lb_fqdn_1
  - name: lb_fqdn_2
  run: |
    set -euo pipefail
    echo "TODO"
    exit 1

    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    cat <<EOF > ./terraform.env
    CLOUDSCALE_API_TOKEN=${INPUT_cloudscale_token}
    TF_VAR_ignition_bootstrap=${INPUT_ignition_bootstrap}
    TF_VAR_lb_cloudscale_api_secret=${INPUT_cloudscale_token_floaty}
    TF_VAR_control_vshn_net_token=${INPUT_control_vshn_api_token}
    GIT_AUTHOR_NAME=$(git config --global user.name)
    GIT_AUTHOR_EMAIL=$(git config --global user.email)
    HIERADATA_REPO_TOKEN=${INPUT_hieradata_repo_token}
    EOF

    tf_image=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.image" \
      dependencies/openshift4-terraform/class/defaults.yml)
    tf_tag=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.tag" \
      dependencies/openshift4-terraform/class/defaults.yml)

    echo "Using Terraform image: ${tf_image}:${tf_tag}"

    base_dir=$(pwd)
    alias terraform='touch .terraformrc; docker run --rm -e REAL_UID=$(id -u) -e TF_CLI_CONFIG_FILE=/tf/.terraformrc --env-file ${base_dir}/terraform.env -w /tf -v $(pwd):/tf --ulimit memlock=-1 "${tf_image}:${tf_tag}" /tf/terraform.sh'

    gitlab_repository_url=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${INPUT_commodore_api_url}/clusters/${INPUT_commodore_cluster_id} | jq -r '.gitRepo.url' | sed 's|ssh://||; s|/|:|')
    gitlab_repository_name=${gitlab_repository_url##*/}
    gitlab_catalog_project_id=$(curl -sH "Authorization: Bearer ${INPUT_gitlab_api_token}" "https://git.vshn.net/api/v4/projects?simple=true&search=${gitlab_repository_name/.git}" | jq -r ".[] | select(.ssh_url_to_repo == \"${gitlab_repository_url}\") | .id")
    gitlab_state_url="https://git.vshn.net/api/v4/projects/${gitlab_catalog_project_id}/terraform/state/cluster"

    pushd catalog/manifests/openshift4-terraform/

    terraform init \
      "-backend-config=address=${gitlab_state_url}" \
      "-backend-config=lock_address=${gitlab_state_url}/lock" \
      "-backend-config=unlock_address=${gitlab_state_url}/lock" \
      "-backend-config=username=${INPUT_gitlab_user_name}" \
      "-backend-config=password=${INPUT_gitlab_api_token}" \
      "-backend-config=lock_method=POST" \
      "-backend-config=unlock_method=DELETE" \
      "-backend-config=retry_wait_min=5"

    cat > override.tf <<EOF
    module "cluster" {
      bootstrap_count          = 0
      master_count             = 0
      infra_count              = 0
      worker_count             = 0
      additional_worker_groups = {}
    }
    EOF
    terraform apply -auto-approve -target "module.cluster.module.lb.module.hiera"

    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    echo "@                                                                                       @"
    echo "@  Please review and merge the LB hieradata MR listed in Terraform output hieradata_mr. @"
    echo "@                                                                                       @"
    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    while (GITLAB_HOST=git.vshn.net GITLAB_TOKEN="${INPUT_gitlab_api_token}" glab mr list -R=appuio/appuio_hieradata | grep "${INPUT_commodore_cluster_id}")
    do
      sleep 10
    done
    echo PR merged, waiting for CI to finish...
    sleep 10
    while (GITLAB_HOST=git.vshn.net GITLAB_TOKEN="${INPUT_gitlab_api_token}" glab mr list -R=appuio/appuio_hieradata | grep "running")
    do
      sleep 10
    done

    terraform apply -auto-approve
    dnstmp=$(mktemp)
    terraform output -raw cluster_dns > "$dnstmp"
    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    echo "@                                                                                       @"
    echo "@  Please add the DNS records shown in the Terraform output to your DNS provider.       @"
    echo "@  Most probably in https://git.vshn.net/vshn/vshn_zonefiles                            @"
    echo "@                                                                                       @"
    echo "@  If terminal selection does not work the entries can also be copied from              @"
    echo "@    $dnstmp                                                                            @"
    echo "@                                                                                       @"
    echo "@  Waiting for record to propagate...                                                   @"
    echo "@                                                                                       @"
    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    while ! (host "api.${INPUT_cluster_domain}")
    do
      sleep 15
    done
    rm -f "$dnstmp"

    lb1=$(terraform state show "module.cluster.module.lb.cloudscale_server.lb[0]" | grep fqdn | awk '{print $2}' | tr -d ' "\r\n')
    lb2=$(terraform state show "module.cluster.module.lb.cloudscale_server.lb[1]" | grep fqdn | awk '{print $2}' | tr -d ' "\r\n')

    echo "Loadbalancer FQDNs: $lb1 , $lb2"

    echo "Waiting for HAproxy ..."
    while true; do
      curl --connect-timeout 1 "http://api.${INPUT_cluster_domain}:6443" &>/dev/null || exit_code=$?
      if [ "$exit_code" -eq 52 ]; then
        echo "  HAproxy up!"
        break
      else
        echo -n "."
        sleep 5
      fi
    done

    echo "updating ssh config..."
    ssh management2.corp.vshn.net "sshop --output-archive /dev/stdout" | tar -C ~ -xzf -
    echo "done"

    echo "waiting for ssh access ..."
    ssh "${lb1}" hostname -f
    ssh "${lb2}" hostname -f

    env -i "lb_fqdn_1=$lb1" >> "$OUTPUT"
    env -i "lb_fqdn_2=$lb2" >> "$OUTPUT"

- match: And I provision the bootstrap node
  description: |-
    This step provisions the bootstrap node for the Exoscale OpenShift cluster using Terraform.
  inputs:
  - name: cloudscale_token
  - name: cloudscale_token_floaty
  - name: control_vshn_api_token
  - name: ignition_bootstrap
  - name: hieradata_repo_token
  - name: gitlab_user_name
  - name: gitlab_api_token
  - name: commodore_cluster_id
  - name: commodore_api_url
  - name: lb_fqdn_1
  - name: lb_fqdn_2
  outputs:
  - name: kubeconfig_path
  run: |
    set -euo pipefail
    echo "TODO"
    exit 1

    export COMMODORE_API_URL="${INPUT_commodore_api_url}"
    installer_dir="$(pwd)/target"

    cat <<EOF > ./terraform.env
    CLOUDSCALE_API_TOKEN=${INPUT_cloudscale_token}
    TF_VAR_ignition_bootstrap=${INPUT_ignition_bootstrap}
    TF_VAR_lb_cloudscale_api_secret=${INPUT_cloudscale_token_floaty}
    TF_VAR_control_vshn_net_token=${INPUT_control_vshn_api_token}
    GIT_AUTHOR_NAME=$(git config --global user.name)
    GIT_AUTHOR_EMAIL=$(git config --global user.email)
    HIERADATA_REPO_TOKEN=${INPUT_hieradata_repo_token}
    EOF

    tf_image=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.image" \
      dependencies/openshift4-terraform/class/defaults.yml)
    tf_tag=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.tag" \
      dependencies/openshift4-terraform/class/defaults.yml)

    echo "Using Terraform image: ${tf_image}:${tf_tag}"

    base_dir=$(pwd)
    alias terraform='touch .terraformrc; docker run --rm -e REAL_UID=$(id -u) -e TF_CLI_CONFIG_FILE=/tf/.terraformrc --env-file ${base_dir}/terraform.env -w /tf -v $(pwd):/tf --ulimit memlock=-1 "${tf_image}:${tf_tag}" /tf/terraform.sh'

    gitlab_repository_url=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${INPUT_commodore_api_url}/clusters/${INPUT_commodore_cluster_id} | jq -r '.gitRepo.url' | sed 's|ssh://||; s|/|:|')
    gitlab_repository_name=${gitlab_repository_url##*/}
    gitlab_catalog_project_id=$(curl -sH "Authorization: Bearer ${INPUT_gitlab_api_token}" "https://git.vshn.net/api/v4/projects?simple=true&search=${gitlab_repository_name/.git}" | jq -r ".[] | select(.ssh_url_to_repo == \"${gitlab_repository_url}\") | .id")
    gitlab_state_url="https://git.vshn.net/api/v4/projects/${gitlab_catalog_project_id}/terraform/state/cluster"

    pushd catalog/manifests/openshift4-terraform/

    terraform init \
      "-backend-config=address=${gitlab_state_url}" \
      "-backend-config=lock_address=${gitlab_state_url}/lock" \
      "-backend-config=unlock_address=${gitlab_state_url}/lock" \
      "-backend-config=username=${INPUT_gitlab_user_name}" \
      "-backend-config=password=${INPUT_gitlab_api_token}" \
      "-backend-config=lock_method=POST" \
      "-backend-config=unlock_method=DELETE" \
      "-backend-config=retry_wait_min=5"

    cat > override.tf <<EOF
    module "cluster" {
      bootstrap_count          = 1
      master_count             = 0
      infra_count              = 0
      worker_count             = 0
      additional_worker_groups = {}
    }
    EOF
    terraform apply -auto-approve

    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    echo "@                                                                                       @"
    echo "@  Please review and merge the LB hieradata MR listed in Terraform output hieradata_mr. @"
    echo "@                                                                                       @"
    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    while (GITLAB_HOST=git.vshn.net GITLAB_TOKEN="${INPUT_gitlab_api_token}" glab mr list -R=appuio/appuio_hieradata | grep "${INPUT_commodore_cluster_id}")
    do
      sleep 10
    done
    echo PR merged, waiting for CI to finish...
    sleep 10
    while (GITLAB_HOST=git.vshn.net GITLAB_TOKEN="${INPUT_gitlab_api_token}" glab mr list -R=appuio/appuio_hieradata | grep "running")
    do
      sleep 10
    done

    ssh "${INPUT_lb_fqdn_1}" sudo puppetctl run
    ssh "${INPUT_lb_fqdn_2}" sudo puppetctl run

    echo -n "Waiting for Bootstrap API to become available .."
    API_URL=$(yq e '.clusters[0].cluster.server' "${installer_dir}/auth/kubeconfig")
    while ! curl --connect-timeout 1 "${API_URL}/healthz" -k &>/dev/null; do
      echo -n "."
      sleep 5
    done && echo "✅ API is up"

    env -i "kubeconfig_path=${installer_dir}/auth/kubeconfig" >> "$OUTPUT"

- match: And I store the subnet ID and floating IP in the Syn hierarchy
  description: |-
    This step retrieves the subnet ID and ingress floating IP from Terraform and
    stores them in the Syn hierarchy.
  inputs:
  - name: cloudscale_token
  - name: cloudscale_token_floaty
  - name: control_vshn_api_token
  - name: ignition_bootstrap
  - name: hieradata_repo_token
  - name: gitlab_user_name
  - name: gitlab_api_token
  - name: commodore_cluster_id
  - name: commodore_tenant_id
  - name: commodore_api_url
  - name: image_major
  - name: image_minor
  run: |
    set -euo pipefail
    echo "TODO"
    exit 1
    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    cat <<EOF > ./terraform.env
    CLOUDSCALE_API_TOKEN=${INPUT_cloudscale_token}
    TF_VAR_ignition_bootstrap=${INPUT_ignition_bootstrap}
    TF_VAR_lb_cloudscale_api_secret=${INPUT_cloudscale_token_floaty}
    TF_VAR_control_vshn_net_token=${INPUT_control_vshn_api_token}
    GIT_AUTHOR_NAME=$(git config --global user.name)
    GIT_AUTHOR_EMAIL=$(git config --global user.email)
    HIERADATA_REPO_TOKEN=${INPUT_hieradata_repo_token}
    EOF

    tf_image=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.image" \
      dependencies/openshift4-terraform/class/defaults.yml)
    tf_tag=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.tag" \
      dependencies/openshift4-terraform/class/defaults.yml)

    echo "Using Terraform image: ${tf_image}:${tf_tag}"

    base_dir=$(pwd)
    alias terraform='touch .terraformrc; docker run --rm -e REAL_UID=$(id -u) -e TF_CLI_CONFIG_FILE=/tf/.terraformrc --env-file ${base_dir}/terraform.env -w /tf -v $(pwd):/tf --ulimit memlock=-1 "${tf_image}:${tf_tag}" /tf/terraform.sh'

    gitlab_repository_url=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${INPUT_commodore_api_url}/clusters/${INPUT_commodore_cluster_id} | jq -r '.gitRepo.url' | sed 's|ssh://||; s|/|:|')
    gitlab_repository_name=${gitlab_repository_url##*/}
    gitlab_catalog_project_id=$(curl -sH "Authorization: Bearer ${INPUT_gitlab_api_token}" "https://git.vshn.net/api/v4/projects?simple=true&search=${gitlab_repository_name/.git}" | jq -r ".[] | select(.ssh_url_to_repo == \"${gitlab_repository_url}\") | .id")
    gitlab_state_url="https://git.vshn.net/api/v4/projects/${gitlab_catalog_project_id}/terraform/state/cluster"

    pushd catalog/manifests/openshift4-terraform/

    terraform init \
      "-backend-config=address=${gitlab_state_url}" \
      "-backend-config=lock_address=${gitlab_state_url}/lock" \
      "-backend-config=unlock_address=${gitlab_state_url}/lock" \
      "-backend-config=username=${INPUT_gitlab_user_name}" \
      "-backend-config=password=${INPUT_gitlab_api_token}" \
      "-backend-config=lock_method=POST" \
      "-backend-config=unlock_method=DELETE" \
      "-backend-config=retry_wait_min=5"

    SUBNET_UUID="$(terraform output -raw subnet_uuid)"
    INGRESS_FLOATING_IP="$(terraform output -raw router_vip)"
    pushd ../../../inventory/classes/${INPUT_commodore_tenant_id}

    yq eval -i '.parameters.openshift.cloudscale.subnet_uuid = "'"$SUBNET_UUID"'"' \
      ${INPUT_commodore_cluster_id}.yml
    yq eval -i '.parameters.openshift.cloudscale.ingress_floating_ip_v4 = "'"$INGRESS_FLOATING_IP"'"' \
      ${INPUT_commodore_cluster_id}.yml

    if not git diff-index --quiet HEAD
    then
      git commit -am "Configure cloudscale subnet UUID and ingress floating IP for ${INPUT_commodore_cluster_id}"
      git push origin master
    fi || true

    popd
    popd # yes, twice.

    # Recompile the catalog
    commodore catalog compile ${INPUT_commodore_cluster_id} --push \
      --dynamic-fact kubernetesVersion.major=1 \
      --dynamic-fact kubernetesVersion.minor="$((INPUT_image_minor+13))" \
      --dynamic-fact openshiftVersion.Major=${INPUT_image_major} \
      --dynamic-fact openshiftVersion.Minor=${INPUT_image_minor}

- match: And I provision the control plane
  description: |-
    This step provisions the control plane nodes with Terraform.
  inputs:
  - name: cloudscale_token
  - name: cloudscale_token_floaty
  - name: control_vshn_api_token
  - name: ignition_bootstrap
  - name: hieradata_repo_token
  - name: gitlab_user_name
  - name: gitlab_api_token
  - name: commodore_cluster_id
  - name: commodore_api_url
  - name: kubeconfig_path
  - name: cluster_domain
  run: |
    set -euo pipefail
    echo "TODO"
    exit 1

    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    cat <<EOF > ./terraform.env
    CLOUDSCALE_API_TOKEN=${INPUT_cloudscale_token}
    TF_VAR_ignition_bootstrap=${INPUT_ignition_bootstrap}
    TF_VAR_lb_cloudscale_api_secret=${INPUT_cloudscale_token_floaty}
    TF_VAR_control_vshn_net_token=${INPUT_control_vshn_api_token}
    GIT_AUTHOR_NAME=$(git config --global user.name)
    GIT_AUTHOR_EMAIL=$(git config --global user.email)
    HIERADATA_REPO_TOKEN=${INPUT_hieradata_repo_token}
    EOF

    tf_image=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.image" \
      dependencies/openshift4-terraform/class/defaults.yml)
    tf_tag=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.tag" \
      dependencies/openshift4-terraform/class/defaults.yml)

    echo "Using Terraform image: ${tf_image}:${tf_tag}"

    base_dir=$(pwd)
    alias terraform='touch .terraformrc; docker run --rm -e REAL_UID=$(id -u) -e TF_CLI_CONFIG_FILE=/tf/.terraformrc --env-file ${base_dir}/terraform.env -w /tf -v $(pwd):/tf --ulimit memlock=-1 "${tf_image}:${tf_tag}" /tf/terraform.sh'

    gitlab_repository_url=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${INPUT_commodore_api_url}/clusters/${INPUT_commodore_cluster_id} | jq -r '.gitRepo.url' | sed 's|ssh://||; s|/|:|')
    gitlab_repository_name=${gitlab_repository_url##*/}
    gitlab_catalog_project_id=$(curl -sH "Authorization: Bearer ${INPUT_gitlab_api_token}" "https://git.vshn.net/api/v4/projects?simple=true&search=${gitlab_repository_name/.git}" | jq -r ".[] | select(.ssh_url_to_repo == \"${gitlab_repository_url}\") | .id")
    gitlab_state_url="https://git.vshn.net/api/v4/projects/${gitlab_catalog_project_id}/terraform/state/cluster"

    pushd catalog/manifests/openshift4-terraform/

    terraform init \
      "-backend-config=address=${gitlab_state_url}" \
      "-backend-config=lock_address=${gitlab_state_url}/lock" \
      "-backend-config=unlock_address=${gitlab_state_url}/lock" \
      "-backend-config=username=${INPUT_gitlab_user_name}" \
      "-backend-config=password=${INPUT_gitlab_api_token}" \
      "-backend-config=lock_method=POST" \
      "-backend-config=unlock_method=DELETE" \
      "-backend-config=retry_wait_min=5"

    cat > override.tf <<EOF
    module "cluster" {
      bootstrap_count          = 1
      infra_count              = 0
      worker_count             = 0
      additional_worker_groups = {}
    }
    EOF

    echo "Running Terraform ..."

    terraform apply -auto-approve
    dnstmp=$(mktemp)
    terraform output -raw cluster_dns > "$dnstmp"
    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    echo "@                                                                                       @"
    echo "@  Please add the etcd DNS records shown in the Terraform output to your DNS provider.  @"
    echo "@  Most probably in https://git.vshn.net/vshn/vshn_zonefiles                            @"
    echo "@                                                                                       @"
    echo "@  If terminal selection does not work the entries can also be copied from              @"
    echo "@    $dnstmp                                                                            @"
    echo "@                                                                                       @"
    echo "@  Waiting for record to propagate...                                                   @"
    echo "@                                                                                       @"
    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    while ! (host "etcd-0.${INPUT_cluster_domain}")
    do
      sleep 15
    done
    rm -f "$dnstmp"

    export KUBECONFIG="${INPUT_kubeconfig_path}"

    echo "Waiting for masters to become ready ..."
    kubectl wait --for create --timeout=600s node -l node-role.kubernetes.io/master
    kubectl wait --for condition=ready --timeout=600s node -l node-role.kubernetes.io/master
    popd

- match: Then I deploy initial manifests
  description: |-
    This step deploys some manifests required during bootstrap, including
    cert-manager,  machine-api-provider, machinesets, loadbalancer controller,
    and ingress loadbalancer.
  inputs:
  - name: commodore_api_url
  - name: vault_address
  - name: kubeconfig_path
  run: |
    set -euo pipefail
    echo "TODO"
    exit 1
    export COMMODORE_API_URL="${INPUT_commodore_api_url}"
    export KUBECONFIG="${INPUT_kubeconfig_path}"

    export VAULT_ADDR=${INPUT_vault_address}
    vault login -method=oidc

    echo '# Applying cert-manager ... #'
    kubectl apply -f catalog/manifests/cert-manager/00_namespace.yaml
    kubectl apply -Rf catalog/manifests/cert-manager/10_cert_manager
    # shellcheck disable=2046
    # we need word splitting here
    kubectl -n syn-cert-manager patch --type=merge \
      $(kubectl -n syn-cert-manager get deploy -oname) \
      -p '{"spec":{"template":{"spec":{"tolerations":[{"operator":"Exists"}]}}}}'
    echo '# Applied cert-manager. #'
    echo
    echo '# Applying machine-api-provider ... #'
    VAULT_TOKEN=$(vault token lookup -format=json | jq -r .data.id)
    export VAULT_TOKEN
    kapitan refs --reveal --refs-path catalog/refs -f catalog/manifests/machine-api-provider-cloudscale/00_secrets.yaml | kubectl apply -f -
    kubectl apply -f catalog/manifests/machine-api-provider-cloudscale/10_clusterRoleBinding.yaml
    kubectl apply -f catalog/manifests/machine-api-provider-cloudscale/10_serviceAccount.yaml
    kubectl apply -f catalog/manifests/machine-api-provider-cloudscale/11_deployment.yaml
    echo '# Applied machine-api-provider. #'
    echo
    echo '# Applying machinesets ... #'
    for f in catalog/manifests/openshift4-nodes/machineset-*.yaml;
      do kubectl apply -f "$f";
    done
    echo '# Applied machinesets. #'
    echo
    echo '# Applying loadbalancer controller ... #'
    kubectl apply -f catalog/manifests/cloudscale-loadbalancer-controller/00_namespace.yaml
    kapitan refs --reveal --refs-path catalog/refs -f catalog/manifests/cloudscale-loadbalancer-controller/10_secrets.yaml | kubectl apply -f -

    # TODO(aa): This fails on the first attempt because likely some of the previous resources need time to come online; figure out what to wait for
    until kubectl apply -Rf catalog/manifests/cloudscale-loadbalancer-controller/10_kustomize
    do
      echo "Manifests didn't apply, waiting a moment to try again ..."
      sleep 20
    done
    echo "Waiting for load balancer to become available ..."
    kubectl -n appuio-cloudscale-loadbalancer-controller \
      wait --for condition=available --timeout 3m \
      deploy cloudscale-loadbalancer-controller-controller-manager
    echo '# Applied loadbalancer controller. #'
    echo
    echo '# Applying ingress loadbalancer ... #'
    kubectl apply -f catalog/manifests/cloudscale-loadbalancer-controller/20_loadbalancers.yaml
    echo '# Applied ingress loadbalancer. #'
    echo

- match: Then I remove the bootstrap node
  description: |-
    After successful bootstrapping, this step removes the bootstrap node again.
  inputs:
  - name: cloudscale_token
  - name: cloudscale_token_floaty
  - name: control_vshn_api_token
  - name: ignition_bootstrap
  - name: hieradata_repo_token
  - name: gitlab_user_name
  - name: gitlab_api_token
  - name: commodore_cluster_id
  - name: commodore_api_url
  - name: lb_fqdn_1
  - name: lb_fqdn_2
  run: |
    set -euo pipefail
    echo "TODO"
    exit 1

    export COMMODORE_API_URL="${INPUT_commodore_api_url}"

    cat <<EOF > ./terraform.env
    CLOUDSCALE_API_TOKEN=${INPUT_cloudscale_token}
    TF_VAR_ignition_bootstrap=${INPUT_ignition_bootstrap}
    TF_VAR_lb_cloudscale_api_secret=${INPUT_cloudscale_token_floaty}
    TF_VAR_control_vshn_net_token=${INPUT_control_vshn_api_token}
    GIT_AUTHOR_NAME=$(git config --global user.name)
    GIT_AUTHOR_EMAIL=$(git config --global user.email)
    HIERADATA_REPO_TOKEN=${INPUT_hieradata_repo_token}
    EOF

    tf_image=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.image" \
      dependencies/openshift4-terraform/class/defaults.yml)
    tf_tag=$(\
      yq eval ".parameters.openshift4_terraform.images.terraform.tag" \
      dependencies/openshift4-terraform/class/defaults.yml)

    echo "Using Terraform image: ${tf_image}:${tf_tag}"

    base_dir=$(pwd)
    alias terraform='touch .terraformrc; docker run --rm -e REAL_UID=$(id -u) -e TF_CLI_CONFIG_FILE=/tf/.terraformrc --env-file ${base_dir}/terraform.env -w /tf -v $(pwd):/tf --ulimit memlock=-1 "${tf_image}:${tf_tag}" /tf/terraform.sh'

    gitlab_repository_url=$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${INPUT_commodore_api_url}/clusters/${INPUT_commodore_cluster_id} | jq -r '.gitRepo.url' | sed 's|ssh://||; s|/|:|')
    gitlab_repository_name=${gitlab_repository_url##*/}
    gitlab_catalog_project_id=$(curl -sH "Authorization: Bearer ${INPUT_gitlab_api_token}" "https://git.vshn.net/api/v4/projects?simple=true&search=${gitlab_repository_name/.git}" | jq -r ".[] | select(.ssh_url_to_repo == \"${gitlab_repository_url}\") | .id")
    gitlab_state_url="https://git.vshn.net/api/v4/projects/${gitlab_catalog_project_id}/terraform/state/cluster"

    pushd catalog/manifests/openshift4-terraform/

    terraform init \
      "-backend-config=address=${gitlab_state_url}" \
      "-backend-config=lock_address=${gitlab_state_url}/lock" \
      "-backend-config=unlock_address=${gitlab_state_url}/lock" \
      "-backend-config=username=${INPUT_gitlab_user_name}" \
      "-backend-config=password=${INPUT_gitlab_api_token}" \
      "-backend-config=lock_method=POST" \
      "-backend-config=unlock_method=DELETE" \
      "-backend-config=retry_wait_min=5"

    rm override.tf
    terraform apply --auto-approve

    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    echo "@                                                                                       @"
    echo "@  Please review and merge the LB hieradata MR listed in Terraform output hieradata_mr. @"
    echo "@                                                                                       @"
    echo "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@"
    while (GITLAB_HOST=git.vshn.net GITLAB_TOKEN="${INPUT_gitlab_api_token}" glab mr list -R=appuio/appuio_hieradata | grep "${INPUT_commodore_cluster_id}")
    do
      sleep 10
    done
    echo PR merged, waiting for CI to finish...
    sleep 10
    while (GITLAB_HOST=git.vshn.net GITLAB_TOKEN="${INPUT_gitlab_api_token}" glab mr list -R=appuio/appuio_hieradata | grep "running")
    do
      sleep 10
    done

    ssh "${INPUT_lb_fqdn_1}" sudo puppetctl run
    ssh "${INPUT_lb_fqdn_2}" sudo puppetctl run

    popd

- match: And I configure initial deployments
  description: |-
    This step configures some deployments that require manual changes after
    cluster bootstrap, such as reverting the Cilium patch from earlier, enabling
    proxy protocol on the Ingress controller, and scheduling the ingress
    controller on the infrastructure nodes.
  inputs:
  - name: commodore_cluster_id
  - name: commodore_api_url
  - name: kubeconfig_path
  run: |
    set -euo pipefail
    echo "TODO"
    exit 1
    export COMMODORE_API_URL="${INPUT_commodore_api_url}"
    export KUBECONFIG="${INPUT_kubeconfig_path}"

    echo '# Enabling proxy protocol ... #'
    kubectl -n openshift-ingress-operator patch ingresscontroller default --type=json \
      -p '[{
        "op":"replace",
        "path":"/spec/endpointPublishingStrategy",
        "value": {"type": "HostNetwork", "hostNetwork": {"protocol": "PROXY"}}
      }]'
    echo '# Enabled proxy protocol. #'
    echo

    distribution="$(curl -sH "Authorization: Bearer $(commodore fetch-token)" ${COMMODORE_API_URL}/clusters/${INPUT_commodore_cluster_id} | jq -r .facts.distribution)"
    if [[ "$distribution" != "oke" ]]
    then
      echo '# Scheduling ingress controller on infra nodes ... #'
      kubectl -n openshift-ingress-operator patch ingresscontroller default --type=json \
        -p '[{
          "op":"replace",
          "path":"/spec/nodePlacement",
          "value":{"nodeSelector":{"matchLabels":{"node-role.kubernetes.io/infra":""}}}
        }]'
      echo '# Scheduled ingress controller on infra nodes. #'
      echo
    fi

    echo '# Removing temporary cert-manager tolerations ... #'
    # shellcheck disable=2046
    # we need word splitting here
    kubectl -n syn-cert-manager patch --type=json \
      $(kubectl -n syn-cert-manager get deploy -oname) \
      -p '[{"op":"remove","path":"/spec/template/spec/tolerations"}]'
    echo '# Removed temporary cert-manager tolerations. #'
